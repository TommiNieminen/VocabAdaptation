{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "04633da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (4.51.3)\n",
      "Requirement already satisfied: filelock in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (from transformers) (2.2.5)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
      "Requirement already satisfied: colorama in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (from requests->transformers) (2025.4.26)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu128\n",
      "Requirement already satisfied: torch in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (2.7.0+cu128)\n",
      "Requirement already satisfied: torchvision in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (0.22.0+cu128)\n",
      "Requirement already satisfied: torchaudio in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (2.7.0+cu128)\n",
      "Requirement already satisfied: filelock in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: numpy in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (from torchvision) (2.2.5)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: sentencepiece in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (0.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers\n",
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128\n",
    "%pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "82246cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import LogitsProcessor, MarianTokenizer, MarianMTModel, AutoModelForCausalLM, AutoTokenizer, StoppingCriteria, StoppingCriteriaList, AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "80682f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name_or_path, device):\n",
    "    config = AutoConfig.from_pretrained(model_name_or_path)\n",
    "\n",
    "    # Check model type by architecture string\n",
    "    if \"Marian\" in config.model_type or config.architectures and any(\"Marian\" in arch for arch in config.architectures):\n",
    "        model = MarianMTModel.from_pretrained(model_name_or_path).to(device).eval()\n",
    "        model_type = \"marian\"\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name_or_path,\n",
    "            torch_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16).to(device).eval()\n",
    "        model_type = \"causal_lm\"\n",
    "\n",
    "    return model, model_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1f988c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenizer(model_name_or_path):\n",
    "    config = AutoConfig.from_pretrained(model_name_or_path)\n",
    "\n",
    "    # Check model type by architecture string\n",
    "    if \"Marian\" in config.model_type or config.architectures and any(\"Marian\" in arch for arch in config.architectures):\n",
    "        tokenizer = MarianTokenizer.from_pretrained(model_name_or_path)\n",
    "    else:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1937f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_tokenize(text, terms, tokenizer, padding_side, device=\"cpu\"):\n",
    "    #TODO: implement batching, add padding according to padding_side parameter\n",
    "    vocab = tokenizer.get_vocab()\n",
    "    text_tokenized = tokenizer(text).input_ids\n",
    "    for term_source,term_target in terms:\n",
    "        term_source_tokenized = tokenizer(term_source).input_ids[:-1]\n",
    "        term_target_tokenized = list(tokenizer(text_target=term_target).input_ids)[:-1]\n",
    "        term_target_tokenized = [vocab[\"augmentsymbol1\"]] + \\\n",
    "            term_target_tokenized + \\\n",
    "            [vocab[\"augmentsymbol2\"]]\n",
    "            \n",
    "        current_aug_part_index = 0\n",
    "        new_text_tokenized = []\n",
    "        for token in text_tokenized:\n",
    "            #TODO: add check for the word continuing\n",
    "            if current_aug_part_index == len(term_source_tokenized):\n",
    "                new_text_tokenized += [vocab[\"augmentsymbol0\"]] + term_source_tokenized + \\\n",
    "                term_target_tokenized\n",
    "                current_aug_part_index = 0\n",
    "            if token == term_source_tokenized[current_aug_part_index]:\n",
    "                current_aug_part_index += 1\n",
    "            elif current_aug_part_index > 1:\n",
    "                new_text_tokenized += term_source_tokenized[0:current_aug_part_index]\n",
    "                new_text_tokenized.append(token)\n",
    "                current_aug_part_index = 0\n",
    "            else:\n",
    "                new_text_tokenized.append(token)\n",
    "        text_tokenized = new_text_tokenized\n",
    "\n",
    "    input_ids = torch.tensor([text_tokenized], device=device)  # batch dimension added\n",
    "    attention_mask = torch.ones_like(input_ids, device=device)\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4bf02a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1945.0933)\n"
     ]
    }
   ],
   "source": [
    "def sum_ignore_inf(tensor):\n",
    "    # Create a mask for finite values (not -inf or inf)\n",
    "    finite_mask = torch.isfinite(tensor)\n",
    "    \n",
    "    # Apply the mask to get only finite values, setting others to 0\n",
    "    finite_values = torch.where(finite_mask, tensor, torch.zeros_like(tensor))\n",
    "    \n",
    "    # Sum all finite values\n",
    "    total_sum = torch.sum(finite_values)\n",
    "    \n",
    "    return total_sum\n",
    "\n",
    "tensor = torch.randn(160, 50000)\n",
    "tensor[0, 0] = -float('inf')  # Add an -inf for testing\n",
    "result = sum_ignore_inf(tensor)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5b5512ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiInputLogitsProcessor(LogitsProcessor):\n",
    "    def __init__(self, models, model_types, tokenizer, models_info, average_mode=\"probs\", only_main_model=False):\n",
    "        self.models = models\n",
    "        self.model_types = model_types\n",
    "        self.tokenizer = tokenizer\n",
    "        self.models_info = models_info\n",
    "        self.average_mode = average_mode\n",
    "        self.current_inputs = {}  # Will store prepared inputs for each model\n",
    "        self.only_main_model = only_main_model\n",
    "\n",
    "    def viking_template(self, sentence):\n",
    "        return f\"<|im_start|>user\\nTranslate into Finnish: {sentence}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "\n",
    "    def marian_llmvoc_template(self, sentence):\n",
    "        return sentence + \"</s>\"\n",
    "\n",
    "    def prepare_inputs(self, src_sentences, num_beams):\n",
    "        \"\"\"Prepare and store model-specific inputs, expanded for beam search\"\"\"\n",
    "        self.current_inputs = {}\n",
    "        batch_size = len(src_sentences)\n",
    "        \n",
    "        for i, (model, model_type) in enumerate(zip(self.models, self.model_types)):\n",
    "            info = self.models_info[i]\n",
    "            \n",
    "            # If this is a Marian with LLM vocab, apply template\n",
    "            if model_type == \"marian\" and not isinstance(self.tokenizer, MarianTokenizer):\n",
    "                templated_src_sentences = [self.marian_llmvoc_template(x) for x in src_sentences]\n",
    "                padding_side = \"right\"\n",
    "            elif model_type != \"marian\":\n",
    "                templated_src_sentences = [self.viking_template(x) for x in src_sentences]\n",
    "                padding_side = \"left\"\n",
    "            else:\n",
    "                templated_src_sentences = src_sentences\n",
    "                padding_side = \"right\"\n",
    "            if info.get(\"terms\"):\n",
    "                inputs = augment_tokenize(\n",
    "                    templated_src_sentences, \n",
    "                    info[\"terms\"], \n",
    "                    self.tokenizer, \n",
    "                    model.device,\n",
    "                    padding_side\n",
    "                )\n",
    "            else:\n",
    "                inputs = self.tokenizer(\n",
    "                    templated_src_sentences, \n",
    "                    return_tensors=\"pt\", \n",
    "                    padding=True,\n",
    "                    truncation=True,\n",
    "                    padding_side=padding_side\n",
    "                ).to(model.device)\n",
    "            \n",
    "            # Expand inputs for beam search, except for main model\n",
    "            if i != 0:\n",
    "                encoder_input_ids = inputs[\"input_ids\"].unsqueeze(1).expand(-1, num_beams, -1)\n",
    "                encoder_input_ids = encoder_input_ids.reshape(batch_size * num_beams, -1)\n",
    "                \n",
    "                attention_mask = inputs[\"attention_mask\"].unsqueeze(1).expand(-1, num_beams, -1)\n",
    "                attention_mask = attention_mask.reshape(batch_size * num_beams, -1)\n",
    "            else:\n",
    "                encoder_input_ids = inputs[\"input_ids\"]\n",
    "                attention_mask = inputs[\"attention_mask\"]\n",
    "                \n",
    "            self.current_inputs[i] = {\n",
    "                \"encoder_input_ids\": encoder_input_ids,\n",
    "                \"attention_mask\": attention_mask,\n",
    "                \"original_batch_size\": batch_size\n",
    "            }\n",
    "\n",
    "    def __call__(self, input_ids, scores):\n",
    "        # TODO: add batched stopping for LLMs based on line break\n",
    "        # Strip input from input_ids for Marian decoding\n",
    "        \n",
    "        if self.only_main_model:\n",
    "            return scores\n",
    "        \n",
    "        avg_probs = self._average_probs(input_ids, scores)\n",
    "        \n",
    "        difference = scores-avg_probs\n",
    "        summed_difference = sum_ignore_inf(difference)\n",
    "        return avg_probs\n",
    "\n",
    "    def _average_probs(self, input_ids, scores):\n",
    "        \"\"\"Average probabilities from all models (log space)\"\"\"\n",
    "        batch_size_times_beams = input_ids.shape[0]\n",
    "        vocab_size = scores.shape[-1]\n",
    "        all_probs = torch.zeros((len(self.models), batch_size_times_beams, vocab_size),\n",
    "                              device=scores.device)\n",
    "        \n",
    "        for i, (model, model_type) in enumerate(zip(self.models, self.model_types)):\n",
    "            if i == 0:\n",
    "                all_probs[i] = torch.exp(scores)\n",
    "            else:\n",
    "                logits = self._get_model_logits(\n",
    "                    model,\n",
    "                    model_type,\n",
    "                    self.current_inputs[i][\"encoder_input_ids\"],\n",
    "                    self.current_inputs[i][\"attention_mask\"],\n",
    "                    input_ids,\n",
    "                    i\n",
    "                )\n",
    "                all_probs[i] = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        mean_log_props = torch.log(all_probs.mean(dim=0))\n",
    "        return mean_log_props\n",
    "        #return torch.logsumexp(all_probs, dim=0) - torch.log(torch.tensor(len(self.models), device=scores.device))\n",
    "\n",
    "    def _get_model_logits(self, model, model_type, encoder_inputs, attention_mask, input_ids, model_idx):\n",
    "        \"\"\"Get logits from a single model\"\"\"\n",
    "        with torch.no_grad():\n",
    "            if model_type == \"marian\":\n",
    "                outputs = model(\n",
    "                    input_ids=encoder_inputs,\n",
    "                    attention_mask=attention_mask,\n",
    "                    decoder_input_ids=input_ids,\n",
    "                )\n",
    "            else:\n",
    "                input_ids_full = torch.cat([encoder_inputs, input_ids], dim=-1)\n",
    "                attention_mask_full = torch.cat([\n",
    "                    attention_mask,\n",
    "                    torch.ones_like(input_ids)\n",
    "                ], dim=-1)\n",
    "                \n",
    "                outputs = model(\n",
    "                    input_ids=input_ids_full,\n",
    "                    attention_mask=attention_mask_full,\n",
    "                )\n",
    "            \n",
    "            logits = outputs.logits[:, -1, :]\n",
    "            logits[:, [self.tokenizer.pad_token_id]] = -float('inf')\n",
    "            return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d6e4a150",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelGroup():\n",
    "    def __init__(self, models_info, tokenizer_path):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.models = []\n",
    "        self.model_types = []\n",
    "        self.tokenizer = load_tokenizer(tokenizer_path)\n",
    "        \n",
    "        for info in models_info:\n",
    "            model, model_type = load_model(info[\"name\"], self.device)\n",
    "            self.models.append(model)\n",
    "            self.model_types.append(model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "dea9c23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShallowFusion:\n",
    "    def __init__(self, models_info, model_group, main_model_idx=0):\n",
    "        self.models_info = models_info\n",
    "        self.main_model_idx = main_model_idx\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.models = model_group.models\n",
    "        self.model_types = model_group.model_types\n",
    "        self.tokenizer = model_group.tokenizer\n",
    "    \n",
    "    def translate(self, src_sentences, num_beams=4, max_length=50, only_main_model=False):\n",
    "        # Initialize logits processor\n",
    "        logits_processor = MultiInputLogitsProcessor(\n",
    "            models=self.models,\n",
    "            model_types=self.model_types,\n",
    "            tokenizer=self.tokenizer,\n",
    "            models_info=self.models_info,\n",
    "            only_main_model=only_main_model)\n",
    "        \n",
    "        # Prepare all model inputs\n",
    "        logits_processor.prepare_inputs(src_sentences, num_beams)\n",
    "        \n",
    "        # Get main model components\n",
    "        main_model = self.models[self.main_model_idx]\n",
    "        main_inputs = logits_processor.current_inputs[self.main_model_idx]\n",
    "        \n",
    "        # Generate with ensemble\n",
    "        if self.model_types[self.main_model_idx] == \"marian\":\n",
    "            outputs = main_model.generate(\n",
    "                input_ids=main_inputs[\"encoder_input_ids\"],\n",
    "                attention_mask=main_inputs[\"attention_mask\"],\n",
    "                num_beams=num_beams,\n",
    "                max_length=max_length,\n",
    "                logits_processor=[logits_processor],\n",
    "                early_stopping=True,\n",
    "                eos_token_id=self.tokenizer.eos_token_id,\n",
    "                use_cache=False,\n",
    "                pad_token_id=self.tokenizer.pad_token_id,\n",
    "            )\n",
    "        else:\n",
    "            outputs = main_model.generate(\n",
    "                input_ids=main_inputs[\"encoder_input_ids\"],\n",
    "                attention_mask=main_inputs[\"attention_mask\"],\n",
    "                num_beams=num_beams,\n",
    "                max_length=max_length,\n",
    "                logits_processor=[logits_processor],\n",
    "                early_stopping=True,\n",
    "                eos_token_id=23,\n",
    "                pad_token_id=self.tokenizer.pad_token_id\n",
    "            )\n",
    "        \n",
    "        return self.tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "82e1e781",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_info = [\n",
    "    {\"name\": \"../termmodel.eng-fin\", \"terms\": None},  # Main model\n",
    "    {\"name\": \"../termmodel.eng-fin\", \"terms\": None},  # Aux model 1\n",
    "]\n",
    "\n",
    "#model_group = ModelGroup(models_info, \"LumiOpen/Viking-7B\")\n",
    "model_group = ModelGroup(models_info, \"../termmodel.eng-fin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a2103e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tämä on testi.', 'Espanjassa oli myrsky.', 'Emme ennakoineet väkijoukkoa.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'base_translations = ensemble.translate(\\n    test_sents,\\n    num_beams=num_beams,\\n    max_length=200,\\n    only_main_model=True\\n)\\nprint(base_translations)'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO: Why does plain generate produce junk but ensemble not? Move on to testing with terms!\n",
    "ensemble = ShallowFusion(models_info, model_group)\n",
    "\n",
    "test_sents = [\"This is a test.\",\"There was a storm in Spain.\",\"We did not anticipate a crowd.\"]\n",
    "num_beams = 8\n",
    "translations = ensemble.translate(\n",
    "    test_sents,\n",
    "    num_beams=num_beams,\n",
    "    max_length=100,\n",
    "    only_main_model=False\n",
    ")\n",
    "\n",
    "print(translations)\n",
    "#for translation in translations:\n",
    "#    print(translation.split(\"\\n\")[-1])\n",
    "\n",
    "\"\"\"base_translations = ensemble.translate(\n",
    "    test_sents,\n",
    "    num_beams=num_beams,\n",
    "    max_length=200,\n",
    "    only_main_model=True\n",
    ")\n",
    "print(base_translations)\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1b9a89b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: ['Palomiehet tulipalossa, jotka johtuivat Shahed-lennokkihyökkäyksestä Ukrainan Punaisen Ristin tukikohtaan']\n",
      "2: ['Tulihälytyshävittäjät, jotka ovat saaneet alkunsa Ukrainan Punaisen Ristin tukikohdan hyökkäyksestä']\n",
      "3: ['Tulihälytyshävittäjät, jotka ovat joutuneet hyökkäykseen Ukrainan Punaiseen Ristiin']\n"
     ]
    }
   ],
   "source": [
    "device=\"cuda\"\n",
    "\n",
    "model, model_type = load_model(\"../termmodel.eng-fin\", device)\n",
    "tokenizer = load_tokenizer(\"../termmodel.eng-fin\")\n",
    "\n",
    "test_sents = [\"Firefighters at blaze caused by a Shahed drone attack on the Ukrainian Red Cross base\"]\n",
    "inputs = tokenizer(\n",
    "                    test_sents, \n",
    "                    return_tensors=\"pt\", \n",
    "                    padding=False,\n",
    "                    truncation=True\n",
    "                ).to(model.device)\n",
    "\n",
    "for i in range(1,4):\n",
    "    output = model.generate(**inputs,num_beams=i)\n",
    "    print(f\"{i}: {tokenizer.batch_decode(output, skip_special_tokens=True)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
