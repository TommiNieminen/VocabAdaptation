{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04633da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (4.51.3)\n",
      "Requirement already satisfied: filelock in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (from transformers) (2.2.5)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
      "Requirement already satisfied: colorama in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (from requests->transformers) (2025.4.26)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu128\n",
      "Requirement already satisfied: torch in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (2.7.0+cu128)\n",
      "Requirement already satisfied: torchvision in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (0.22.0+cu128)\n",
      "Requirement already satisfied: torchaudio in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (2.7.0+cu128)\n",
      "Requirement already satisfied: filelock in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: numpy in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (from torchvision) (2.2.5)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (0.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers\n",
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128\n",
    "%pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82246cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\niemi\\Documents\\Testing\\Cuda\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import LogitsProcessor, MarianTokenizer, MarianMTModel, AutoModelForCausalLM, AutoTokenizer, StoppingCriteria, StoppingCriteriaList, AutoConfig, BeamSearchScorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80682f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name_or_path, device):\n",
    "    config = AutoConfig.from_pretrained(model_name_or_path)\n",
    "\n",
    "    # Check model type by architecture string\n",
    "    if \"Marian\" in config.model_type or config.architectures and any(\"Marian\" in arch for arch in config.architectures):\n",
    "        model = MarianMTModel.from_pretrained(model_name_or_path).to(device).eval()\n",
    "        model_type = \"marian\"\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name_or_path,\n",
    "            torch_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16).to(device).eval()\n",
    "        model_type = \"causal_lm\"\n",
    "\n",
    "    return model, model_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f988c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenizer(model_name_or_path):\n",
    "    config = AutoConfig.from_pretrained(model_name_or_path)\n",
    "\n",
    "    # Check model type by architecture string\n",
    "    if \"Marian\" in config.model_type or config.architectures and any(\"Marian\" in arch for arch in config.architectures):\n",
    "        tokenizer = MarianTokenizer.from_pretrained(model_name_or_path)\n",
    "    else:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f89a237a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_tokenize_batched(texts, terms_list, tokenizer, padding_side, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Process batched input of texts and terms for augmentation.\n",
    "    \n",
    "    Args:\n",
    "        texts: List of input text strings\n",
    "        terms_list: List of lists of (source_term, target_term) tuples for each text\n",
    "        tokenizer: Tokenizer to use\n",
    "        padding_side: \"left\" or \"right\" - where to add padding\n",
    "        device: Device to put tensors on\n",
    "    \"\"\"\n",
    "    vocab = tokenizer.get_vocab()\n",
    "    all_input_ids = []\n",
    "    \n",
    "    for text, terms in zip(texts, terms_list):\n",
    "        text_tokenized = tokenizer(text).input_ids\n",
    "        \n",
    "        for term_source, term_target in terms:\n",
    "            term_source_tokenized = tokenizer(term_source).input_ids[:-1]  # remove EOS\n",
    "            term_target_tokenized = tokenizer(text_target=term_target).input_ids[:-1]  # remove EOS\n",
    "            term_target_tokenized = [vocab[\"augmentsymbol1\"]] + term_target_tokenized + [vocab[\"augmentsymbol2\"]]\n",
    "            \n",
    "            current_aug_part_index = 0\n",
    "            new_text_tokenized = []\n",
    "            \n",
    "            for token in text_tokenized:\n",
    "                # Check if we're in the middle of matching a term\n",
    "                if current_aug_part_index < len(term_source_tokenized) and token == term_source_tokenized[current_aug_part_index]:\n",
    "                    current_aug_part_index += 1\n",
    "\n",
    "                    # If we fully matched a term\n",
    "                    if current_aug_part_index == len(term_source_tokenized):\n",
    "                        new_text_tokenized.extend([vocab[\"augmentsymbol0\"]] + term_source_tokenized + term_target_tokenized)\n",
    "                        current_aug_part_index = 0\n",
    "                    continue\n",
    "                \n",
    "                # If we partially matched but then failed\n",
    "                if current_aug_part_index > 0:\n",
    "                    new_text_tokenized.extend(term_source_tokenized[:current_aug_part_index])\n",
    "                    current_aug_part_index = 0\n",
    "                \n",
    "                # Default case - just add the token\n",
    "                new_text_tokenized.append(token)\n",
    "            \n",
    "            text_tokenized = new_text_tokenized\n",
    "        \n",
    "        all_input_ids.append(text_tokenized)\n",
    "    \n",
    "    # Padding and batching\n",
    "    max_len = max(len(ids) for ids in all_input_ids)\n",
    "    padded_input_ids = []\n",
    "    attention_masks = []\n",
    "    \n",
    "    for ids in all_input_ids:\n",
    "        pad_len = max_len - len(ids)\n",
    "        attention_mask = [1] * len(ids) + [0] * pad_len\n",
    "        \n",
    "        if padding_side == \"right\":\n",
    "            padded_ids = ids + [tokenizer.pad_token_id] * pad_len\n",
    "        else:\n",
    "            padded_ids = [tokenizer.pad_token_id] * pad_len + ids\n",
    "            attention_mask = [0] * pad_len + attention_mask[:len(ids)]\n",
    "        \n",
    "        padded_input_ids.append(padded_ids)\n",
    "        attention_masks.append(attention_mask)\n",
    "    \n",
    "    # Convert to tensors\n",
    "    input_ids = torch.tensor(padded_input_ids, device=device)\n",
    "    attention_mask = torch.tensor(attention_masks, device=device)\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd1937f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_tokenize(text, terms, tokenizer, padding_side, device=\"cpu\"):\n",
    "    #TODO: implement batching, add padding according to padding_side parameter\n",
    "    vocab = tokenizer.get_vocab()\n",
    "    text_tokenized = tokenizer(text).input_ids\n",
    "    for term_source,term_target in terms:\n",
    "        term_source_tokenized = tokenizer(term_source).input_ids[:-1]\n",
    "        term_target_tokenized = list(tokenizer(text_target=term_target).input_ids)[:-1]\n",
    "        term_target_tokenized = [vocab[\"augmentsymbol1\"]] + \\\n",
    "            term_target_tokenized + \\\n",
    "            [vocab[\"augmentsymbol2\"]]\n",
    "            \n",
    "        current_aug_part_index = 0\n",
    "        new_text_tokenized = []\n",
    "        for token in text_tokenized:\n",
    "            #TODO: add check for the word continuing\n",
    "            if current_aug_part_index == len(term_source_tokenized):\n",
    "                new_text_tokenized += [vocab[\"augmentsymbol0\"]] + term_source_tokenized + \\\n",
    "                term_target_tokenized\n",
    "                current_aug_part_index = 0\n",
    "            if token == term_source_tokenized[current_aug_part_index]:\n",
    "                current_aug_part_index += 1\n",
    "            elif current_aug_part_index > 1:\n",
    "                new_text_tokenized += term_source_tokenized[0:current_aug_part_index]\n",
    "                new_text_tokenized.append(token)\n",
    "                current_aug_part_index = 0\n",
    "            else:\n",
    "                new_text_tokenized.append(token)\n",
    "        text_tokenized = new_text_tokenized\n",
    "\n",
    "    input_ids = torch.tensor([text_tokenized], device=device)\n",
    "    attention_mask = torch.ones_like(input_ids, device=device)\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4bf02a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1361.8593)\n"
     ]
    }
   ],
   "source": [
    "def sum_ignore_inf(tensor):\n",
    "    # Create a mask for finite values (not -inf or inf)\n",
    "    finite_mask = torch.isfinite(tensor)\n",
    "    \n",
    "    # Apply the mask to get only finite values, setting others to 0\n",
    "    finite_values = torch.where(finite_mask, tensor, torch.zeros_like(tensor))\n",
    "    \n",
    "    # Sum all finite values\n",
    "    total_sum = torch.sum(finite_values)\n",
    "    \n",
    "    return total_sum\n",
    "\n",
    "tensor = torch.randn(160, 50000)\n",
    "tensor[0, 0] = -float('inf')  # Add an -inf for testing\n",
    "result = sum_ignore_inf(tensor)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "7ed7455a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def combine_with_guide_softmax(\n",
    "    softmax_tensor: torch.Tensor,\n",
    "    tokenizer,\n",
    "    emphasis_strength: float = 10.0\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Combines model softmaxes using the last model as a guide.\n",
    "    Emphasizes models whose symbol probabilities differ significantly from the guide.\n",
    "\n",
    "    Args:\n",
    "        softmax_tensor: Tensor of shape (num_models, num_positions, num_symbols).\n",
    "        tokenizer: Tokenizer object with a `decode()` method.\n",
    "        emphasis_strength: Controls how strongly differences from the guide affect weighting.\n",
    "\n",
    "    Returns:\n",
    "        Combined softmax tensor of shape (num_positions, num_symbols).\n",
    "    \"\"\"\n",
    "    guide = softmax_tensor[-1]  # Shape: (num_positions, num_symbols)\n",
    "    others = softmax_tensor[:-1]  # Shape: (num_models - 1, num_positions, num_symbols)\n",
    "\n",
    "    # Compute absolute difference from guide\n",
    "    diff = torch.abs(others - guide.unsqueeze(0))  # Shape: (num_models - 1, num_positions, num_symbols)\n",
    "\n",
    "    # Higher difference => higher emphasis\n",
    "    weights = F.softmax(emphasis_strength * diff, dim=0)  # (num_models - 1, num_positions, num_symbols)\n",
    "\n",
    "    # Weighted sum over other models\n",
    "    combined = torch.sum(weights * others, dim=0)  # (num_positions, num_symbols)\n",
    "\n",
    "    # Optionally include the guide in the final combination\n",
    "    # For example: combine 90% weighted other models + 10% guide\n",
    "    combined = 0.9 * combined + 0.1 * guide\n",
    "\n",
    "    # Normalize to ensure valid softmax (numerical safety)\n",
    "    combined = combined / combined.sum(dim=-1, keepdim=True)\n",
    "    \n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d584329c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def weighted_softmax_combine(softmax_tensor: torch.Tensor, tokenizer, temperature: float = 1.0) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Emphasizes symbols that have significantly higher probabilities in one model compared to others.\n",
    "\n",
    "    Args:\n",
    "        softmax_tensor: Tensor of shape (num_models, num_positions, num_symbols),\n",
    "                        where num_positions = num_beams * batch_size.\n",
    "        temperature: Controls sharpness of weighting; lower => more emphasis on dominant symbols.\n",
    "\n",
    "    Returns:\n",
    "        Combined softmax tensor of shape (num_positions, num_symbols).\n",
    "    \"\"\"\n",
    "    # TODO: check what the probs of \"kotelo\" and \"toimeksianto\" are, see if there's a pronounced jump\n",
    "\n",
    "    # Compute mean and std across models for each symbol\n",
    "    mean_probs = softmax_tensor.mean(dim=0, keepdim=True)  # Shape: (1, num_positions, num_symbols)\n",
    "    std_probs = softmax_tensor.std(dim=0, keepdim=True)    # Shape: (1, num_positions, num_symbols)\n",
    "\n",
    "    # Calculate \"dominance score\" per model per symbol: how much it exceeds the mean\n",
    "    dominance_score = (softmax_tensor - mean_probs) / (std_probs + 1e-8)  # Z-score like\n",
    "    \n",
    "    # Apply softmax over models (dim=0) for each symbol, using dominance score\n",
    "    weights = F.softmax(dominance_score / temperature, dim=0)  # Shape: (num_models, num_positions, num_symbols)\n",
    "\n",
    "    # Weighted sum across models\n",
    "    combined_softmax = torch.sum(weights * softmax_tensor, dim=0)  # Shape: (num_positions, num_symbols)\n",
    "\n",
    "    # Top 10 symbols by average weight\n",
    "    topk = torch.topk(weights, k=10)\n",
    "    top_indices = topk.indices.tolist()\n",
    "    top_values = topk.values.tolist()\n",
    "    \"\"\"\n",
    "    print(\"Top 10 emphasized symbols:\")\n",
    "    for idx, score in zip(top_indices[1], top_values[1]):\n",
    "        symbol = tokenizer.decode(idx)\n",
    "        print(f\"Token ID {str(idx)}: '{symbol}' (weight: {str(score)})\")\"\"\"\n",
    "\n",
    "    for test_word in [\"rasia\",\"laatikko\",\"ruutu\",\"kotelo\",\"paketti\",\"pakkaus\"]:\n",
    "        test_index = tokenizer(text_target=test_word).input_ids[0]\n",
    "        for i in range(0,len(softmax_tensor)):\n",
    "            test_prob_pos = softmax_tensor[i][0][test_index]\n",
    "            if test_prob_pos.item() > 0.1:\n",
    "                print(f\"{test_word} {i}: \" + str(test_prob_pos))\n",
    "        test_prob_pos = combined_softmax[0][test_index]\n",
    "        print(f\"{test_word} combined: \" + str(test_prob_pos))\n",
    "    \n",
    "    return combined_softmax\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "5b5512ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiInputLogitsProcessor(LogitsProcessor):\n",
    "    def __init__(self, models, model_types, tokenizer, models_info, only_main_model=False, num_beams=1):\n",
    "        self.models = models\n",
    "        self.model_types = model_types\n",
    "        self.tokenizer = tokenizer\n",
    "        self.models_info = models_info\n",
    "        self.current_inputs = {}  # Will store prepared inputs for each model\n",
    "        self.only_main_model = only_main_model\n",
    "        self.num_beams = num_beams\n",
    "\n",
    "    def viking_template(self, sentence):\n",
    "        return f\"<|im_start|>user\\nTranslate into Finnish: {sentence}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "\n",
    "    def marian_llmvoc_template(self, sentence):\n",
    "        return sentence + \"</s>\"\n",
    "\n",
    "    def prepare_inputs(self, src_and_terms, num_beams):\n",
    "        \"\"\"Prepare and store model-specific inputs, expanded for beam search\"\"\"\n",
    "        self.current_inputs = {}\n",
    "        batch_size = len(src_and_terms[0])\n",
    "        \n",
    "        for i, (model, model_type, src_and_terms_for_model) in enumerate(zip(self.models, self.model_types, src_and_terms)):\n",
    "            src_sentences, terms = zip(*src_and_terms_for_model)\n",
    "            \n",
    "            # If this is a Marian model with LLM vocab, apply template to fix LLM tokenizer\n",
    "            # differences with expected Marian tokenization \n",
    "            if model_type == \"marian\" and not isinstance(self.tokenizer, MarianTokenizer):\n",
    "                templated_src_sentences = [self.marian_llmvoc_template(x) for x in src_sentences]\n",
    "                padding_side = \"right\"\n",
    "            elif model_type != \"marian\":\n",
    "                templated_src_sentences = [self.viking_template(x) for x in src_sentences]\n",
    "                padding_side = \"left\"\n",
    "            else:\n",
    "                templated_src_sentences = src_sentences\n",
    "                padding_side = \"right\"\n",
    "            if any(terms):\n",
    "                # inputs = augment_tokenize(\n",
    "                inputs = augment_tokenize_batched(\n",
    "                    templated_src_sentences, \n",
    "                    terms, \n",
    "                    self.tokenizer, \n",
    "                    padding_side,\n",
    "                    model.device\n",
    "                )\n",
    "            else:\n",
    "                inputs = self.tokenizer(\n",
    "                    templated_src_sentences, \n",
    "                    return_tensors=\"pt\", \n",
    "                    padding=True,\n",
    "                    truncation=True,\n",
    "                    padding_side=padding_side\n",
    "                ).to(model.device)\n",
    "            \n",
    "            # Expand inputs for beam search, except for main model (that already gets\n",
    "            # expanded by HF)\n",
    "            if i != 0:\n",
    "                encoder_input_ids = inputs[\"input_ids\"].unsqueeze(1).expand(-1, num_beams, -1)\n",
    "                encoder_input_ids = encoder_input_ids.reshape(batch_size * num_beams, -1)\n",
    "                \n",
    "                attention_mask = inputs[\"attention_mask\"].unsqueeze(1).expand(-1, num_beams, -1)\n",
    "                attention_mask = attention_mask.reshape(batch_size * num_beams, -1)\n",
    "            else:\n",
    "                encoder_input_ids = inputs[\"input_ids\"]\n",
    "                attention_mask = inputs[\"attention_mask\"]\n",
    "                \n",
    "            self.current_inputs[i] = {\n",
    "                \"encoder_input_ids\": encoder_input_ids,\n",
    "                \"attention_mask\": attention_mask,\n",
    "                \"original_batch_size\": batch_size\n",
    "            }\n",
    "            \n",
    "            print(self.tokenizer.batch_decode(encoder_input_ids))\n",
    "\n",
    "    def __call__(self, input_ids, scores):\n",
    "        # TODO: test with two aux models, not using the main model at all. Will the terms work there?\n",
    "        if self.only_main_model:\n",
    "            # Why does this return non-sense, when averaging identical input sentence outputs\n",
    "            # does not?\n",
    "            return scores\n",
    "        \n",
    "        avg_probs = self._average_probs(input_ids, scores)\n",
    "        \n",
    "        difference = scores-avg_probs\n",
    "        summed_difference = sum_ignore_inf(difference)\n",
    "        print(summed_difference)\n",
    "        top5 = torch.topk(avg_probs,5)\n",
    "        return avg_probs\n",
    "\n",
    "    def _average_probs(self, input_ids, scores):\n",
    "        \"\"\"Average probabilities from all models (log space)\"\"\"\n",
    "        batch_size_times_beams = input_ids.shape[0]\n",
    "        vocab_size = scores.shape[-1]\n",
    "        all_probs = torch.zeros((len(self.models), batch_size_times_beams, vocab_size),\n",
    "                              device=scores.device)\n",
    "        \n",
    "        for i, (model, model_type) in enumerate(zip(self.models, self.model_types)):\n",
    "            if i == 0:\n",
    "                all_probs[i] = torch.exp(scores)\n",
    "            else:\n",
    "                logits = self._get_model_logits(\n",
    "                    model,\n",
    "                    model_type,\n",
    "                    self.current_inputs[i][\"encoder_input_ids\"],\n",
    "                    self.current_inputs[i][\"attention_mask\"],\n",
    "                    input_ids,\n",
    "                    i\n",
    "                )\n",
    "                all_probs[i] = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        \n",
    "        #remove the main model tensor (it behaves weirdly)\n",
    "        all_probs = all_probs[1:, :, :]\n",
    "        \n",
    "        #mean_log_probs = torch.log(all_probs.mean(dim=0))\n",
    "        \n",
    "        #mean_log_probs = torch.log(weighted_softmax_combine(all_probs, self.tokenizer))\n",
    "        mean_log_probs = torch.log(combine_with_guide_softmax(all_probs, self.tokenizer))\n",
    "        \n",
    "        return mean_log_probs\n",
    "        #return torch.logsumexp(all_probs, dim=0) - torch.log(torch.tensor(len(self.models), device=scores.device))\n",
    "\n",
    "    def _get_model_logits(self, model, model_type, encoder_inputs, attention_mask, input_ids, model_idx):\n",
    "        \"\"\"Get logits from a single model\"\"\"\n",
    "        with torch.no_grad():\n",
    "            if model_type == \"marian\":\n",
    "                outputs = model(\n",
    "                    input_ids=encoder_inputs,\n",
    "                    attention_mask=attention_mask,\n",
    "                    decoder_input_ids=input_ids,\n",
    "                )\n",
    "            else:\n",
    "                input_ids_full = torch.cat([encoder_inputs, input_ids], dim=-1)\n",
    "                attention_mask_full = torch.cat([\n",
    "                    attention_mask,\n",
    "                    torch.ones_like(input_ids)\n",
    "                ], dim=-1)\n",
    "                \n",
    "                outputs = model(\n",
    "                    input_ids=input_ids_full,\n",
    "                    attention_mask=attention_mask_full,\n",
    "                )\n",
    "            \n",
    "            logits = outputs.logits[:, -1, :]\n",
    "            logits[:, [self.tokenizer.pad_token_id]] = -float('inf')\n",
    "            return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6e4a150",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelGroup():\n",
    "    def __init__(self, models_info, tokenizer_path):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.models = []\n",
    "        self.model_types = []\n",
    "        self.tokenizer = load_tokenizer(tokenizer_path)\n",
    "        \n",
    "        for info in models_info:\n",
    "            model, model_type = load_model(info[\"name\"], self.device)\n",
    "            self.models.append(model)\n",
    "            self.model_types.append(model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dea9c23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShallowFusion:\n",
    "    def __init__(self, models_info, model_group, main_model_idx=0):\n",
    "        self.models_info = models_info\n",
    "        self.main_model_idx = main_model_idx\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.models = model_group.models\n",
    "        self.model_types = model_group.model_types\n",
    "        self.tokenizer = model_group.tokenizer\n",
    "    \n",
    "    def translate(self, src_and_terms, num_beams=4, max_length=50, only_main_model=False):\n",
    "        # Initialize logits processor\n",
    "        logits_processor = MultiInputLogitsProcessor(\n",
    "            models=self.models,\n",
    "            model_types=self.model_types,\n",
    "            tokenizer=self.tokenizer,\n",
    "            models_info=self.models_info,\n",
    "            only_main_model=only_main_model,\n",
    "            num_beams=num_beams)\n",
    "        \n",
    "        # Prepare all model inputs\n",
    "        logits_processor.prepare_inputs(src_and_terms, num_beams)\n",
    "        \n",
    "        # Get main model components\n",
    "        main_model = self.models[self.main_model_idx]\n",
    "        main_inputs = logits_processor.current_inputs[self.main_model_idx]\n",
    "        \n",
    "        # Generate with ensemble\n",
    "        if self.model_types[self.main_model_idx] == \"marian\":\n",
    "            outputs = main_model.generate(\n",
    "                input_ids=main_inputs[\"encoder_input_ids\"],\n",
    "                attention_mask=main_inputs[\"attention_mask\"],\n",
    "                num_beams=num_beams,\n",
    "                max_length=max_length,\n",
    "                logits_processor=[logits_processor],\n",
    "                early_stopping=True,\n",
    "                eos_token_id=self.tokenizer.eos_token_id,\n",
    "                use_cache=False,\n",
    "                pad_token_id=self.tokenizer.pad_token_id,\n",
    "            )\n",
    "        else:\n",
    "            outputs = main_model.generate(\n",
    "                input_ids=main_inputs[\"encoder_input_ids\"],\n",
    "                attention_mask=main_inputs[\"attention_mask\"],\n",
    "                num_beams=num_beams,\n",
    "                max_length=max_length,\n",
    "                logits_processor=[logits_processor],\n",
    "                early_stopping=True,\n",
    "                eos_token_id=23,\n",
    "                pad_token_id=self.tokenizer.pad_token_id\n",
    "            )\n",
    "        \n",
    "        return self.tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "82e1e781",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\niemi\\Documents\\Testing\\Cuda\\.venv\\Lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "models_info = [\n",
    "    {\"name\": \"../termmodel.eng-fin\", \"terms\": None},  # Main model\n",
    "    {\"name\": \"../termmodel.eng-fin\", \"terms\": None},  # Aux model 1\n",
    "    {\"name\": \"../termmodel.eng-fin\", \"terms\": None},  # Aux model 2\n",
    "    {\"name\": \"../termmodel.eng-fin\", \"terms\": None},  # Aux model 3\n",
    "]\n",
    "\n",
    "#model_group = ModelGroup(models_info, \"LumiOpen/Viking-7B\")\n",
    "model_group = ModelGroup(models_info, tokenizer_path=\"../termmodel.eng-fin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebc2def",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\niemi\\Documents\\Testing\\Cuda\\.venv\\Lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "models_info = [\n",
    "    {\"name\": \"Helsinki-NLP/opus-mt-en-fi\", \"terms\": None},  # Main model\n",
    "    {\"name\": \"Helsinki-NLP/opus-mt-en-fi\", \"terms\": None},  # Aux model 1\n",
    "    {\"name\": \"Helsinki-NLP/opus-mt-en-fi\", \"terms\": None},  # Aux model 2\n",
    "]\n",
    "\n",
    "#model_group = ModelGroup(models_info, \"LumiOpen/Viking-7B\")\n",
    "model_group = ModelGroup(models_info, tokenizer_path=\"Helsinki-NLP/opus-mt-en-fi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a2103e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Put theaugmentsymbol0 packageaugmentsymbol1 pakettiaugmentsymbol2 into the box.</s>']\n",
      "['Put theaugmentsymbol0 packageaugmentsymbol1 pakkausaugmentsymbol2 into the box.</s>', 'Put theaugmentsymbol0 packageaugmentsymbol1 pakkausaugmentsymbol2 into the box.</s>', 'Put theaugmentsymbol0 packageaugmentsymbol1 pakkausaugmentsymbol2 into the box.</s>', 'Put theaugmentsymbol0 packageaugmentsymbol1 pakkausaugmentsymbol2 into the box.</s>']\n",
      "['Put the package into theaugmentsymbol0 boxaugmentsymbol1 rasiaaugmentsymbol2.</s>', 'Put the package into theaugmentsymbol0 boxaugmentsymbol1 rasiaaugmentsymbol2.</s>', 'Put the package into theaugmentsymbol0 boxaugmentsymbol1 rasiaaugmentsymbol2.</s>', 'Put the package into theaugmentsymbol0 boxaugmentsymbol1 rasiaaugmentsymbol2.</s>']\n",
      "['Put the package into the box.</s>', 'Put the package into the box.</s>', 'Put the package into the box.</s>', 'Put the package into the box.</s>']\n",
      "tensor(-15740.3564, device='cuda:0')\n",
      "tensor(-147071.4062, device='cuda:0')\n",
      "tensor(-582959.5000, device='cuda:0')\n",
      "tensor(-715421., device='cuda:0')\n",
      "tensor(-520584.2188, device='cuda:0')\n",
      "tensor(-2751996.7500, device='cuda:0')\n",
      "tensor(-1785505.7500, device='cuda:0')\n",
      "['Laita pakkaus rasiaan.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'base_translations = ensemble.translate(\\n    test_sents,\\n    num_beams=num_beams,\\n    max_length=200,\\n    only_main_model=True\\n)\\nprint(base_translations)'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble = ShallowFusion(models_info, model_group)\n",
    "\n",
    "test_sents1 = [(\"Put the package into the box.\",[(\"package\",\"paketti\")])]\n",
    "test_sents2 = [(\"Put the package into the box.\",[(\"package\",\"pakkaus\")])]\n",
    "test_sents3 = [(\"Put the package into the box.\",[(\"box\",\"kotelo\")])]\n",
    "test_sents4 = [(\"Put the package into the box.\",[])]\n",
    "#test_sents1 = [(\"This is a test.\",None)]\n",
    "#test_sents2 = [(\"This is a test.\",None)]\n",
    "#test_sents3 = [(\"This is a test.\",None)]\n",
    "test_sents = [test_sents1,test_sents2,test_sents3,test_sents4]\n",
    "num_beams = 4\n",
    "translations = ensemble.translate(\n",
    "    test_sents,\n",
    "    num_beams=num_beams,\n",
    "    max_length=100,\n",
    "    only_main_model=False\n",
    ")\n",
    "\n",
    "print(translations)\n",
    "#for translation in translations:\n",
    "#    print(translation.split(\"\\n\")[-1])\n",
    "\n",
    "\"\"\"base_translations = ensemble.translate(\n",
    "    test_sents,\n",
    "    num_beams=num_beams,\n",
    "    max_length=200,\n",
    "    only_main_model=True\n",
    ")\n",
    "print(base_translations)\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1b9a89b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: ['Omena on punainen ja tuoli sininen.']\n",
      "2: ['Omena on punainen ja tuoli sininen.']\n",
      "3: ['Omena on punainen ja tuoli sininen.']\n",
      "4: ['Omena on punainen ja tuoli sininen.']\n",
      "5: ['Omena on punainen ja tuoli sininen.']\n",
      "6: ['Omena on punainen ja tuoli sininen.']\n",
      "7: ['Omena on punainen ja tuoli sininen.']\n"
     ]
    }
   ],
   "source": [
    "device=\"cuda\"\n",
    "\n",
    "model, model_type = load_model(\"Helsinki-NLP/opus-mt-en-fi\", device)\n",
    "tokenizer = load_tokenizer(\"Helsinki-NLP/opus-mt-en-fi\")\n",
    "\n",
    "test_sents = [\"The apple is red, and the chair is blue.\"]\n",
    "inputs = tokenizer(\n",
    "                    test_sents, \n",
    "                    return_tensors=\"pt\", \n",
    "                    padding=False,\n",
    "                    truncation=True\n",
    "                ).to(model.device)\n",
    "\n",
    "for i in range(1,8):\n",
    "    output = model.generate(**inputs,num_beams=i)\n",
    "    print(f\"{i}: {tokenizer.batch_decode(output, skip_special_tokens=True)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
