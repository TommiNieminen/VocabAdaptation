{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b5bdcee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (4.51.3)\n",
      "Requirement already satisfied: filelock in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (from transformers) (2.2.5)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
      "Requirement already satisfied: colorama in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (from requests->transformers) (2025.4.26)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu128\n",
      "Requirement already satisfied: torch in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (2.7.0+cu128)\n",
      "Requirement already satisfied: torchvision in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (0.22.0+cu128)\n",
      "Requirement already satisfied: torchaudio in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (2.7.0+cu128)\n",
      "Requirement already satisfied: filelock in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: numpy in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (from torchvision) (2.2.5)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\users\\niemi\\documents\\testing\\cuda\\.venv\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers\n",
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46461baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\niemi\\Documents\\Testing\\Cuda\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import MarianTokenizer, MarianMTModel, AutoModelForCausalLM, AutoTokenizer, StoppingCriteria, StoppingCriteriaList, AutoConfig\n",
    "from transformers.generation import BeamSearchScorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87ff9ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def augment_tokenize(text, terms, tokenizer, device=\"cpu\"):\n",
    "    vocab = tokenizer.get_vocab()\n",
    "    text_tokenized = tokenizer(text).input_ids\n",
    "    for term_source,term_target in terms:\n",
    "        term_source_tokenized = tokenizer(term_source).input_ids[:-1]\n",
    "        term_target_tokenized = list(tokenizer(text_target=term_target).input_ids)[:-1]\n",
    "        term_target_tokenized = [vocab[\"augmentsymbol1\"]] + \\\n",
    "            term_target_tokenized + \\\n",
    "            [vocab[\"augmentsymbol2\"]]\n",
    "            \n",
    "        current_aug_part_index = 0\n",
    "        new_text_tokenized = []\n",
    "        for token in text_tokenized:\n",
    "            #TODO: add check for the word continuing\n",
    "            if current_aug_part_index == len(term_source_tokenized):\n",
    "                new_text_tokenized += [vocab[\"augmentsymbol0\"]] + term_source_tokenized + \\\n",
    "                term_target_tokenized\n",
    "                current_aug_part_index = 0\n",
    "            if token == term_source_tokenized[current_aug_part_index]:\n",
    "                current_aug_part_index += 1\n",
    "            elif current_aug_part_index > 1:\n",
    "                new_text_tokenized += term_source_tokenized[0:current_aug_part_index]\n",
    "                new_text_tokenized.append(token)\n",
    "                current_aug_part_index = 0\n",
    "            else:\n",
    "                new_text_tokenized.append(token)\n",
    "        text_tokenized = new_text_tokenized\n",
    "\n",
    "    input_ids = torch.tensor([text_tokenized], device=device)  # batch dimension added\n",
    "    attention_mask = torch.ones_like(input_ids, device=device)\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4aabf724",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name_or_path, device):\n",
    "    config = AutoConfig.from_pretrained(model_name_or_path)\n",
    "\n",
    "    # Check model type by architecture string\n",
    "    if \"Marian\" in config.model_type or config.architectures and any(\"Marian\" in arch for arch in config.architectures):\n",
    "        model = MarianMTModel.from_pretrained(model_name_or_path).to(device).eval()\n",
    "        model_type = \"marian\"\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name_or_path,\n",
    "            torch_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16).to(device).eval()\n",
    "        model_type = \"causal_lm\"\n",
    "\n",
    "    return model, model_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4a311df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenizer(model_name_or_path):\n",
    "    config = AutoConfig.from_pretrained(model_name_or_path)\n",
    "\n",
    "    # Check model type by architecture string\n",
    "    if \"Marian\" in config.model_type or config.architectures and any(\"Marian\" in arch for arch in config.architectures):\n",
    "        tokenizer = MarianTokenizer.from_pretrained(model_name_or_path)\n",
    "    else:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d433a60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Add batching for beam search, so that different beams are generated as batches.\n",
    "# Also add batching for inputs, so that sets of sentences can be processed as a batch.\n",
    "# Add past_key_ids to speed up LLM inference. \n",
    "\n",
    "#TODO: The vocab fixed model is being trained on Puhti, test that with LLM. Try training on puhti\n",
    "# without --tsv (that caused some errors before), to see if the logical epoch weirdness (epoch being just 1M\n",
    "# sentences) has an effect (is it maybe only training on the first million sentence pairs over and over\n",
    "# again?).\n",
    "\n",
    "#TODO: Start working with the pipeline, fine-tuning RAT models with the same vocab as the term model. Also\n",
    "# do term models with single term only, to test ensembling performance.\n",
    "\n",
    "class ShallowFusion():\n",
    "    def __init__(self, models_info):\n",
    "        self.models_info = models_info\n",
    "        # === Load tokenizer and models ===\n",
    "        # The tokenizer of the first model is used\n",
    "        self.tokenizer = load_tokenizer(models_info[0][\"name\"])\n",
    "        \n",
    "        # tokenizer = AutoTokenizer.from_pretrained(\"LumiOpen/Viking-7B\")\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.init_models()\n",
    "\n",
    "    def init_models(self):\n",
    "        self.models = []        \n",
    "        for info in self.models_info:\n",
    "            model, model_type = load_model(info[\"name\"], self.device)\n",
    "            self.models.append((model,model_type))\n",
    "\n",
    "    def initialize_inputs(self, src_sentences_per_model):\n",
    "        self.encoder_input_ids_list = []\n",
    "        self.attention_masks_list = []\n",
    "        self.decoder_input_ids_list = []\n",
    "\n",
    "        num_beams = 4\n",
    "        self.eos_token_id = self.tokenizer.eos_token_id\n",
    "        self.pad_token_id = self.tokenizer.pad_token_id  # Marian uses pad token to start decoding\n",
    "        self.start_token_id = self.tokenizer.pad_token_id  # Marian uses pad token to start decoding\n",
    "\n",
    "        for (sentence,terms),(model,model_type) in zip(src_sentences_per_model,self.models):\n",
    "            # Tokenize and prepare inputs\n",
    "            if terms:\n",
    "                enc_inputs = augment_tokenize(sentence, terms, self.tokenizer, self.device)\n",
    "            else:\n",
    "                enc_inputs = self.tokenizer(sentence, return_tensors=\"pt\").to(self.device)\n",
    "                \n",
    "            encoder_input_ids = enc_inputs[\"input_ids\"].expand(num_beams, -1).clone()\n",
    "            attention_mask = enc_inputs[\"attention_mask\"].expand(num_beams, -1).clone()\n",
    "\n",
    "            self.encoder_input_ids_list.append(encoder_input_ids)\n",
    "            self.attention_masks_list.append(attention_mask)\n",
    "\n",
    "            if model_type == \"marian\":\n",
    "                # Init decoder_input_ids with a start token\n",
    "                decoder_input_ids = torch.full(\n",
    "                    (num_beams, 1),\n",
    "                    fill_value=self.start_token_id,\n",
    "                    dtype=torch.long,\n",
    "                    device=self.device,\n",
    "                )\n",
    "            else:\n",
    "                # LLMs keep track of decoder input ids, but concat them with the input for generation\n",
    "                decoder_input_ids = torch.empty((num_beams, 0), dtype=torch.long, device=self.device)\n",
    "            self.decoder_input_ids_list.append(decoder_input_ids)\n",
    "    \n",
    "    def translate(self, src_sentences_per_model, num_beams=4, max_length=50):\n",
    "        self.initialize_inputs(src_sentences_per_model)\n",
    "        \n",
    "        # === Beam search setup ===\n",
    "        beam_scorer = BeamSearchScorer(\n",
    "            batch_size=1,\n",
    "            num_beams=num_beams,\n",
    "            device=self.device,\n",
    "            length_penalty=1.0,\n",
    "            do_early_stopping=True,\n",
    "            num_beam_hyps_to_keep=num_beams,\n",
    "        )\n",
    "\n",
    "        beam_scores = torch.zeros((num_beams,), dtype=torch.float, device=self.device)\n",
    "        beam_scores[1:] = -1e9  # Only first beam is active at the beginning\n",
    "\n",
    "        cur_len = 1  # decoder_input_ids starts with 1 token\n",
    "\n",
    "        # === Step-by-step beam search loop ===\n",
    "        while cur_len < max_length:\n",
    "            all_log_probs = []\n",
    "\n",
    "            # === Each model provides logits from its own source + decoder input ===\n",
    "            for (model,model_type), encoder_input_ids, attention_mask, decoder_input_ids in zip(\n",
    "                self.models, self.encoder_input_ids_list, self.attention_masks_list, self.decoder_input_ids_list\n",
    "            ):\n",
    "                with torch.no_grad():\n",
    "                    if model_type == \"marian\":\n",
    "                        outputs = model(\n",
    "                            input_ids=encoder_input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            decoder_input_ids=decoder_input_ids,\n",
    "                        )\n",
    "                        logits = outputs.logits[:, -1, :]  # (num_beams, vocab_size)\n",
    "                        logits[:, [self.pad_token_id]] = float(\"-inf\")\n",
    "                    else:\n",
    "                        input_ids = torch.cat([encoder_input_ids, decoder_input_ids], dim=-1)\n",
    "                        outputs = model(\n",
    "                            input_ids=input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                        )\n",
    "                        logits = outputs.logits[:, -1, :]  # (num_beams, vocab_size)\n",
    "\n",
    "                    log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "                    all_log_probs.append(log_probs)\n",
    "\n",
    "            # === Shallow fusion: average log_probs ===\n",
    "            avg_log_probs = torch.stack(all_log_probs).mean(dim=0)\n",
    "\n",
    "            # === Beam step ===\n",
    "            next_beam_scores, next_tokens = torch.topk(avg_log_probs, 2, dim=1)\n",
    "            next_beam_scores += beam_scores[:, None]\n",
    "\n",
    "            next_beam_scores = next_beam_scores.view(1, -1)\n",
    "            next_tokens = next_tokens.view(1, -1)\n",
    "            next_indices = torch.arange(num_beams, device=self.device).repeat_interleave(2).view(1, -1)\n",
    "\n",
    "            decoder_input_ids_for_process = self.decoder_input_ids_list[0]\n",
    "            \n",
    "            beam_outputs = beam_scorer.process(\n",
    "                decoder_input_ids_for_process,\n",
    "                next_beam_scores,\n",
    "                next_tokens,\n",
    "                next_indices,\n",
    "                eos_token_id=self.eos_token_id,\n",
    "            )\n",
    "\n",
    "            # === Update decoder_input_ids and beam scores ===\n",
    "            for i in range(len(self.models)):\n",
    "                self.decoder_input_ids_list[i] = torch.cat(\n",
    "                    [\n",
    "                        self.decoder_input_ids_list[i][beam_outputs.data[\"next_beam_indices\"]],\n",
    "                        beam_outputs.data[\"next_beam_tokens\"].unsqueeze(-1),\n",
    "                    ],\n",
    "                    dim=-1,\n",
    "                )\n",
    "\n",
    "            beam_scores = beam_outputs.data[\"next_beam_scores\"]\n",
    "            cur_len += 1\n",
    "\n",
    "            if beam_scorer.is_done:\n",
    "                break\n",
    "\n",
    "        # === Finalize hypotheses ===\n",
    "        final_outputs = beam_scorer.finalize(\n",
    "            self.decoder_input_ids_list[0],\n",
    "            beam_scores,\n",
    "            final_beam_tokens=None,\n",
    "            final_beam_indices=None,\n",
    "            max_length=cur_len,\n",
    "            pad_token_id=self.pad_token_id,\n",
    "            eos_token_id=self.eos_token_id\n",
    "        )\n",
    "\n",
    "        # === Decode translations ===\n",
    "        translation = [self.tokenizer.decode(t, skip_special_tokens=False) for t in final_outputs.data[\"sequences\"][0]]\n",
    "        translations = self.tokenizer.batch_decode(final_outputs.data[\"sequences\"], skip_special_tokens=True,)\n",
    "        return translations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d93026a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchedShallowFusion():\n",
    "    def __init__(self, models_info):\n",
    "        self.models_info = models_info\n",
    "        # === Load tokenizer and models ===\n",
    "        # The tokenizer of the first model is used\n",
    "        self.tokenizer = load_tokenizer(models_info[0][\"name\"])\n",
    "        \n",
    "        # tokenizer = AutoTokenizer.from_pretrained(\"LumiOpen/Viking-7B\")\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.init_models()\n",
    "\n",
    "    def init_models(self):\n",
    "        self.models = []        \n",
    "        for info in self.models_info:\n",
    "            model, model_type = load_model(info[\"name\"], self.device)\n",
    "            self.models.append((model,model_type))\n",
    "\n",
    "    def initialize_inputs(self, src_sentences_per_model, batch_size, num_beams):\n",
    "        self.encoder_input_ids_list = []\n",
    "        self.attention_masks_list = []\n",
    "        self.decoder_input_ids_list = []\n",
    "\n",
    "        self.eos_token_id = self.tokenizer.eos_token_id\n",
    "        self.pad_token_id = self.tokenizer.pad_token_id  # Marian uses pad token to start decoding\n",
    "        self.start_token_id = self.tokenizer.pad_token_id  # Marian uses pad token to start decoding\n",
    "\n",
    "        for (sentences, terms_list), (model, model_type) in zip(src_sentences_per_model, self.models):\n",
    "            # Tokenize and prepare inputs for all sentences in the batch\n",
    "            if terms_list and terms_list[0]:  # Check if terms are provided\n",
    "                # Assuming augment_tokenize can handle batch processing or needs to be modified\n",
    "                enc_inputs = [augment_tokenize(sentence, terms, self.tokenizer, self.device) \n",
    "                              for sentence, terms in zip(sentences, terms_list)]\n",
    "                encoder_input_ids = torch.cat([x[\"input_ids\"] for x in enc_inputs], dim=0)\n",
    "                attention_mask = torch.cat([x[\"attention_mask\"] for x in enc_inputs], dim=0)\n",
    "            else:\n",
    "                enc_inputs = self.tokenizer(sentences, return_tensors=\"pt\", padding=True, truncation=True).to(self.device)\n",
    "                encoder_input_ids = enc_inputs[\"input_ids\"]\n",
    "                attention_mask = enc_inputs[\"attention_mask\"]\n",
    "            \n",
    "            # Expand for beam search\n",
    "            encoder_input_ids = encoder_input_ids.unsqueeze(1).expand(-1, num_beams, -1).reshape(batch_size * num_beams, -1)\n",
    "            attention_mask = attention_mask.unsqueeze(1).expand(-1, num_beams, -1).reshape(batch_size * num_beams, -1)\n",
    "\n",
    "            self.encoder_input_ids_list.append(encoder_input_ids)\n",
    "            self.attention_masks_list.append(attention_mask)\n",
    "\n",
    "            if model_type == \"marian\":\n",
    "                # Init decoder_input_ids with a start token for each beam in each batch\n",
    "                decoder_input_ids = torch.full(\n",
    "                    (batch_size * num_beams, 1),\n",
    "                    fill_value=self.start_token_id,\n",
    "                    dtype=torch.long,\n",
    "                    device=self.device,\n",
    "                )\n",
    "            else:\n",
    "                # LLMs keep track of decoder input ids, but concat them with the input for generation\n",
    "                decoder_input_ids = torch.empty((batch_size * num_beams, 0), dtype=torch.long, device=self.device)\n",
    "            self.decoder_input_ids_list.append(decoder_input_ids)\n",
    "    \n",
    "    def translate(self, src_sentences_per_model, num_beams=4, max_length=50):\n",
    "        # Determine batch size from input\n",
    "        batch_size = len(src_sentences_per_model[0][0])\n",
    "        self.initialize_inputs(src_sentences_per_model, batch_size, num_beams)\n",
    "        \n",
    "        # === Beam search setup ===\n",
    "        beam_scorer = BeamSearchScorer(\n",
    "            batch_size=batch_size,\n",
    "            num_beams=num_beams,\n",
    "            device=self.device,\n",
    "            length_penalty=1.0,\n",
    "            do_early_stopping=True,\n",
    "            num_beam_hyps_to_keep=num_beams,\n",
    "        )\n",
    "\n",
    "        beam_scores = torch.zeros((batch_size, num_beams), dtype=torch.float, device=self.device)\n",
    "        beam_scores[:, 1:] = -1e9  # Only first beam is active at the beginning\n",
    "        beam_scores = beam_scores.view(-1)  # Flatten to (batch_size * num_beams,)\n",
    "\n",
    "        cur_len = 1  # decoder_input_ids starts with 1 token\n",
    "\n",
    "        # === Step-by-step beam search loop ===\n",
    "        while cur_len < max_length:\n",
    "            all_log_probs = []\n",
    "\n",
    "            # === Each model provides logits from its own source + decoder input ===\n",
    "            for (model, model_type), encoder_input_ids, attention_mask, decoder_input_ids in zip(\n",
    "                self.models, self.encoder_input_ids_list, self.attention_masks_list, self.decoder_input_ids_list\n",
    "            ):\n",
    "                with torch.no_grad():\n",
    "                    if model_type == \"marian\":\n",
    "                        outputs = model(\n",
    "                            input_ids=encoder_input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            decoder_input_ids=decoder_input_ids,\n",
    "                        )\n",
    "                        logits = outputs.logits[:, -1, :]  # (batch_size * num_beams, vocab_size)\n",
    "                        logits[:, [self.pad_token_id]] = float(\"-inf\")\n",
    "                    else:\n",
    "                        input_ids = torch.cat([encoder_input_ids, decoder_input_ids], dim=-1)\n",
    "                        outputs = model(\n",
    "                            input_ids=input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                        )\n",
    "                        logits = outputs.logits[:, -1, :]  # (batch_size * num_beams, vocab_size)\n",
    "\n",
    "                    log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "                    all_log_probs.append(log_probs)\n",
    "\n",
    "            # === Shallow fusion: average log_probs ===\n",
    "            avg_log_probs = torch.stack(all_log_probs).mean(dim=0)\n",
    "\n",
    "            # === Beam step ===\n",
    "            next_beam_scores, next_tokens = torch.topk(avg_log_probs, 2, dim=1)\n",
    "            \n",
    "            # Reshape scores and tokens for beam processing\n",
    "            next_beam_scores = next_beam_scores.view(batch_size, num_beams * 2)\n",
    "            next_tokens = next_tokens.view(batch_size, num_beams * 2)\n",
    "            \n",
    "            # Prepare beam scores for addition\n",
    "            beam_scores = beam_scores.view(batch_size, num_beams)\n",
    "            beam_scores = beam_scores.unsqueeze(-1).expand(-1, -1, 2).reshape(batch_size, num_beams * 2)\n",
    "            next_beam_scores = next_beam_scores + beam_scores\n",
    "\n",
    "            # Prepare for beam scorer\n",
    "            next_indices = torch.arange(num_beams, device=self.device).repeat(2 * batch_size).view(batch_size, -1)\n",
    "\n",
    "            # the shape is incorrect, should be batch_size * num_beams\n",
    "            decoder_input_ids_for_process = self.decoder_input_ids_list[0]\n",
    "            \n",
    "            beam_outputs = beam_scorer.process(\n",
    "                decoder_input_ids_for_process,\n",
    "                next_beam_scores,\n",
    "                next_tokens,\n",
    "                next_indices,\n",
    "                eos_token_id=self.eos_token_id,\n",
    "                pad_token_id=self.pad_token_id\n",
    "            )\n",
    "\n",
    "            # === Update decoder_input_ids and beam scores ===\n",
    "            for i in range(len(self.models)):\n",
    "                self.decoder_input_ids_list[i] = torch.cat(\n",
    "                    [\n",
    "                        self.decoder_input_ids_list[i][beam_outputs.data[\"next_beam_indices\"]],\n",
    "                        beam_outputs.data[\"next_beam_tokens\"].unsqueeze(-1),\n",
    "                    ],\n",
    "                    dim=-1,\n",
    "                )\n",
    "                \n",
    "                # Update the attention mask for LLMs (no need to update Marian masks, since they use\n",
    "                # the same input masks throughout)\n",
    "                if self.models[i][1] != \"marian\":\n",
    "                    new_mask = torch.ones_like(beam_outputs.data[\"next_beam_tokens\"].unsqueeze(-1))\n",
    "                    self.attention_masks_list[i] = torch.cat([\n",
    "                        self.attention_masks_list[i],\n",
    "                        new_mask\n",
    "                    ], dim=-1)\n",
    "\n",
    "            beam_scores = beam_outputs.data[\"next_beam_scores\"].view(-1)\n",
    "            cur_len += 1\n",
    "\n",
    "            if beam_scorer.is_done.all():\n",
    "                break\n",
    "\n",
    "        # === Finalize hypotheses ===\n",
    "        final_outputs = beam_scorer.finalize(\n",
    "            self.decoder_input_ids_list[0],\n",
    "            beam_scores,\n",
    "            final_beam_tokens=None,\n",
    "            final_beam_indices=None,\n",
    "            max_length=cur_len,\n",
    "            pad_token_id=self.pad_token_id,\n",
    "            eos_token_id=self.eos_token_id\n",
    "        )\n",
    "\n",
    "        # === Decode translations ===\n",
    "        translations = self.tokenizer.batch_decode(final_outputs.data[\"sequences\"], skip_special_tokens=True)\n",
    "        return translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c047a1ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\niemi\\Documents\\Testing\\Cuda\\.venv\\Lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "# === Define models and their unique source inputs ===\n",
    "models_info = [\n",
    "    {\n",
    "        \"name\": \"LumiOpen/Viking-7B\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"../converted-vocab_fix_model_viking7\",\n",
    "    },\n",
    "]\n",
    "\n",
    "#shallow_fusion = BatchedShallowFusion(models_info)\n",
    "shallow_fusion = ShallowFusion(models_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b8ed08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viking_template(sentence):\n",
    "    return (f\"<|im_start|>user\\nTranslate into Finnish: {sentence}<|im_end|>\\n<|im_start|>assistant\\n\",[])\n",
    "\n",
    "def marian_llmvoc_template(sentence):\n",
    "    return (sentence + \"</s>\", [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0f0935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('<|im_start|>user\\nTranslate into Finnish: Firefighters at blaze caused by a Shahed drone attack on the Ukrainian Red Cross base.<|im_end|>\\n<|im_start|>assistant\\n',), ([],)], [('Firefighters at blaze caused by a Shahed drone attack on the Ukrainian Red Cross base.</s>',), ([],)]]\n",
      "\n",
      "🌍 Shallow Fusion Translations (Multi-Input):\n",
      "Palomiehet, jotka ovat joutuneet hyökkäyksestä Ukrainan Punaisen Ristin tukikohtaan.\n",
      "Palomiehet, jotka ovat joutuneet hyökkäyksestä Ukrainan Punaisen Ristin tukikohtaan.[2]\n",
      "Palomiehet, jotka ovat joutuneet hyökkäyksestä Ukrainan Punaisen Ristin tukikohtaan.[1]\n",
      "Palomiehet, jotka ovat joutuneet hyökkäyksestä Ukrainan Punaisen Ristin tukikohtaan.[1][2]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"PYDEVD_WARN_SLOW_RESOLVE_TIMEOUT\"] = \"100\"\n",
    "\n",
    "# TODO: Test using custom logits processor\n",
    "\n",
    "\"\"\"test_sents = [\n",
    "    \"This is a test.\",\n",
    "    \"The ice was melting.\",\n",
    "    \"Tunnels shudder as the animals fly through them.\",\n",
    "    \"Tunnels shudder as the bats fly through them.\"]\"\"\"\n",
    "\n",
    "test_sents = [\n",
    "    \"Tunnels shudder as the bats fly through them.\"]\n",
    "input_sentences = [\n",
    "    list(zip(*[viking_template(test_sent) for test_sent in test_sents])),\n",
    "    list(zip(*[marian_llmvoc_template(test_sent) for test_sent in test_sents]))\n",
    "]\n",
    "print(input_sentences)\n",
    "#translations = shallow_fusion.translate(input_sentences)\n",
    "translations = shallow_fusion.translate((viking_template(test_sents[0]), marian_llmvoc_template(test_sents[0])))\n",
    "\n",
    "print(\"\\n🌍 Shallow Fusion Translations (Multi-Input):\")\n",
    "print(\"\\n\".join(translations))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
