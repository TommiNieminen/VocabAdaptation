{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46461baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\niemi\\Documents\\Testing\\Cuda\\hf_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import MarianTokenizer, MarianMTModel, AutoModelForCausalLM, AutoTokenizer, StoppingCriteria, StoppingCriteriaList\n",
    "from transformers.generation import BeamSearchScorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1b3b06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "term_model = MarianMTModel.from_pretrained(\"D:/Users/niemi/Documents/Testing/Cuda/termmodel.eng-fin\")\n",
    "term_tokenizer = MarianTokenizer.from_pretrained(\"D:/Users/niemi/Documents/Testing/Cuda/termmodel.eng-fin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5f1872f",
   "metadata": {},
   "outputs": [],
   "source": [
    "viking_vocab_model = MarianMTModel.from_pretrained(\"D:/Users/niemi/Documents/Testing/Cuda/converted-vocab_fix_model_viking7\")\n",
    "viking_tokenizer = AutoTokenizer.from_pretrained(\"LumiOpen/Viking-7B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2a834ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  4.77it/s]\n"
     ]
    }
   ],
   "source": [
    "branch = \"1000B\"\n",
    "viking_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"LumiOpen/Viking-7B\",\n",
    "    torch_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,\n",
    "    revision=branch,\n",
    ").to(\"cuda\")\n",
    "\n",
    "viking_tokenizer = AutoTokenizer.from_pretrained(\"LumiOpen/Viking-7B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3ea4b798",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StoppingCriteriaSub(StoppingCriteria):\n",
    "    def __init__(self,tokenizer, stops = [], encounters=1):\n",
    "        super().__init__()\n",
    "        self.stops = [stop.to(\"cuda\") for stop in stops]\n",
    "        self.tokenizer = tokenizer\n",
    "        self.batch_stop_list = []\n",
    "\n",
    "    def __call__(self, batch_input_ids: torch.LongTensor, scores: torch.FloatTensor):\n",
    "        source_id = 0\n",
    "        for input_ids in batch_input_ids:\n",
    "            if source_id in self.batch_stop_list:\n",
    "                source_id += 1\n",
    "                continue\n",
    "            last_token = input_ids[-1]\n",
    "            for stop in self.stops:\n",
    "                if self.tokenizer.decode(stop) in self.tokenizer.decode(last_token):\n",
    "                    print(\"stop:\",self.tokenizer.decode(stop))\n",
    "                    print(\"last token:\",self.tokenizer.decode(last_token))\n",
    "                    self.batch_stop_list.append(source_id)\n",
    "            source_id += 1\n",
    "        if len(self.batch_stop_list) == len(batch_input_ids):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "# Stop at line break\n",
    "stop_words = [\"<|im_end|>\"]\n",
    "stop_words_ids = [viking_tokenizer(stop_word, return_tensors='pt', add_special_tokens=False)['input_ids'].squeeze() for stop_word in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "69e64c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stop: <|im_end|>\n",
      "last token: <|im_end|>\n",
      "<|im_start|>user\n",
      "Translate into Finnish: The Sheilasâ€™ Wheels owner, Esure, will be sold to the Belgian insurer Ageas.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Sheilas' Wheelsin omistaja Esure myydÃ¤Ã¤n belgialaiselle vakuutusyhtiÃ¶lle Ageasille.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "stopping_criteria = StoppingCriteriaList([StoppingCriteriaSub(viking_tokenizer, stops=stop_words_ids)])\n",
    "inputs = viking_tokenizer(\"<|im_start|>user\\nTranslate into Finnish: The Sheilasâ€™ Wheels owner, Esure, will be sold to the Belgian insurer Ageas.<|im_end|>\\n<|im_start|>assistant\\n\",return_tensors=\"pt\").to(\"cuda\")\n",
    "translated = viking_model.generate(**inputs,max_length=200, stopping_criteria=stopping_criteria)\n",
    "print(viking_tokenizer.decode(translated[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "98b34259",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'term_tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mterm_tokenizer\u001b[49m.get_vocab()[\u001b[33m\"\u001b[39m\u001b[33maugmentsymbol1\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[31mNameError\u001b[39m: name 'term_tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "print(term_tokenizer.get_vocab()[\"augmentsymbol1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ff9ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def augment_tokenize(text, terms, tokenizer, device=\"cpu\"):\n",
    "    vocab = tokenizer.get_vocab()\n",
    "    text_tokenized = tokenizer(text).input_ids\n",
    "    for term_source,term_target in terms:\n",
    "        term_source_tokenized = tokenizer(term_source).input_ids[:-1]\n",
    "        term_target_tokenized = list(tokenizer(text_target=term_target).input_ids)[:-1]\n",
    "        term_target_tokenized = [vocab[\"augmentsymbol1\"]] + \\\n",
    "            term_target_tokenized + \\\n",
    "            [vocab[\"augmentsymbol2\"]]\n",
    "            \n",
    "        current_aug_part_index = 0\n",
    "        new_text_tokenized = []\n",
    "        for token in text_tokenized:\n",
    "            #TODO: add check for the word continuing\n",
    "            if current_aug_part_index == len(term_source_tokenized):\n",
    "                new_text_tokenized += [vocab[\"augmentsymbol0\"]] + term_source_tokenized + \\\n",
    "                term_target_tokenized\n",
    "                current_aug_part_index = 0\n",
    "            if token == term_source_tokenized[current_aug_part_index]:\n",
    "                current_aug_part_index += 1\n",
    "            elif current_aug_part_index > 1:\n",
    "                new_text_tokenized += term_source_tokenized[0:current_aug_part_index]\n",
    "                new_text_tokenized.append(token)\n",
    "                current_aug_part_index = 0\n",
    "            else:\n",
    "                new_text_tokenized.append(token)\n",
    "        text_tokenized = new_text_tokenized\n",
    "\n",
    "    input_ids = torch.tensor([text_tokenized], device=device)  # batch dimension added\n",
    "    attention_mask = torch.ones_like(input_ids, device=device)\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41be608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[57829, 57829, 48685, 57440, 45603, 57432, 34377, 57291, 36046, 48683,\n",
      "         57440, 48558, 57432, 24547, 57291,    46, 41756]])\n",
      "<pad> <pad> Theaugmentsymbol0 studentaugmentsymbol1 oppilasaugmentsymbol2 passed theaugmentsymbol0 testaugmentsymbol1 koeaugmentsymbol2.</s>\n",
      "{'input_ids': tensor([[48685,  6090, 16089, 30124, 55986, 28752,  7250,   275, 45603,  6126,\n",
      "         41755, 55986, 28752,  7250,   446, 34235, 36996,  5567, 51683, 30124,\n",
      "         55986, 28752,  7250,  1177, 36046, 48683, 48558,    46, 41756]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1]])}\n",
      "Testin lÃ¤pÃ¤isi auppsymbol1 oppilaseugmentsymbol2 lÃ¤pÃ¤istyÃ¤Ã¤n tutkimuksensa, jonka tulokset olivat samat kuin muidenkin opiskelijoiden ja opiskelijoiden, joiden tutkimustulokset olivat samat kuin opiskelijoiden ja opiskelijoiden, joiden tutkimustulokset olivat samat kuin opiskelijoiden ja opiskelijoiden, joiden tutkimustulokset olivat samat kuin opiskelijoiden ja opiskelijoiden, joiden tutkimustulokset olivat samat kuin muiden opiskelijoiden, ja jotka olivat samat kuin opiskelijoiden ja opiskelijoiden, joiden tutkimustulokset olivat samat kuin muiden opiskelijoiden, ja jotka olivat\n"
     ]
    }
   ],
   "source": [
    "terms = [(\"student\",\"oppilas\"),(\"test\",\"koe\")]\n",
    "inputs = augment_tokenize(\"The student passed the test.\", terms, term_tokenizer)\n",
    "print(inputs[\"input_ids\"])\n",
    "print(term_tokenizer.decode(inputs[\"input_ids\"][0]))\n",
    "inputs1 = term_tokenizer(\"The student passed the test.\",return_tensors=\"pt\")\n",
    "print(inputs1)\n",
    "# Generate translation with max length\n",
    "translated = term_model.generate(**inputs, max_length=100, num_beams=12)\n",
    "\n",
    "# Decode the translation\n",
    "translated_text = term_tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "print(translated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8e1c460",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     31\u001b[39m start_token_id = tokenizer.pad_token_id  \u001b[38;5;66;03m# Marian uses pad token to start decoding\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m info \u001b[38;5;129;01min\u001b[39;00m models_info:\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m     model = \u001b[43mMarianMTModel\u001b[49m.from_pretrained(info[\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m]).to(device).eval()\n\u001b[32m     35\u001b[39m     models.append(model)\n\u001b[32m     37\u001b[39m     \u001b[38;5;66;03m# Tokenize and prepare inputs\u001b[39;00m\n\u001b[32m     38\u001b[39m     \u001b[38;5;66;03m#enc_inputs = tokenizer(info[\"src_text\"], return_tensors=\"pt\").to(device)\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     31\u001b[39m start_token_id = tokenizer.pad_token_id  \u001b[38;5;66;03m# Marian uses pad token to start decoding\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m info \u001b[38;5;129;01min\u001b[39;00m models_info:\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m     model = \u001b[43mMarianMTModel\u001b[49m.from_pretrained(info[\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m]).to(device).eval()\n\u001b[32m     35\u001b[39m     models.append(model)\n\u001b[32m     37\u001b[39m     \u001b[38;5;66;03m# Tokenize and prepare inputs\u001b[39;00m\n\u001b[32m     38\u001b[39m     \u001b[38;5;66;03m#enc_inputs = tokenizer(info[\"src_text\"], return_tensors=\"pt\").to(device)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_pydevd_bundle\\\\pydevd_cython.pyx:1697\u001b[39m, in \u001b[36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_pydevd_bundle\\\\pydevd_cython.pyx:634\u001b[39m, in \u001b[36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_pydevd_bundle\\\\pydevd_cython.pyx:1112\u001b[39m, in \u001b[36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_pydevd_bundle\\\\pydevd_cython.pyx:1090\u001b[39m, in \u001b[36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_pydevd_bundle\\\\pydevd_cython.pyx:494\u001b[39m, in \u001b[36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\niemi\\Documents\\Testing\\Cuda\\hf_env\\Lib\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2188\u001b[39m, in \u001b[36mPyDB.do_wait_suspend\u001b[39m\u001b[34m(self, thread, frame, event, arg, exception_type)\u001b[39m\n\u001b[32m   2185\u001b[39m             from_this_thread.append(frame_custom_thread_id)\n\u001b[32m   2187\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._threads_suspended_single_notification.notify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[32m-> \u001b[39m\u001b[32m2188\u001b[39m         keep_suspended = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrace_suspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2190\u001b[39m frames_list = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2192\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[32m   2193\u001b[39m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\niemi\\Documents\\Testing\\Cuda\\hf_env\\Lib\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2257\u001b[39m, in \u001b[36mPyDB._do_wait_suspend\u001b[39m\u001b[34m(self, thread, frame, event, arg, trace_suspend_type, from_this_thread, frames_tracker)\u001b[39m\n\u001b[32m   2254\u001b[39m                 queue.put(internal_cmd)\n\u001b[32m   2255\u001b[39m                 wait_timeout = TIMEOUT_FAST\n\u001b[32m-> \u001b[39m\u001b[32m2257\u001b[39m         \u001b[43mnotify_event\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2258\u001b[39m         notify_event.clear()\n\u001b[32m   2260\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\threading.py:629\u001b[39m, in \u001b[36mEvent.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    627\u001b[39m signaled = \u001b[38;5;28mself\u001b[39m._flag\n\u001b[32m    628\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[32m--> \u001b[39m\u001b[32m629\u001b[39m     signaled = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cond\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    630\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\threading.py:331\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m         gotit = \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    332\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    333\u001b[39m         gotit = waiter.acquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# === Define models and their unique source inputs ===\n",
    "models_info = [\n",
    "    \n",
    "    {\n",
    "        \"name\": \"D:/Users/niemi/Documents/Testing/Cuda/converted-vocab_fix_model_viking7\",\n",
    "        \"src_text\": \"When Gemma Lucy Smart received an invitation to attend a conference in the US, she was excited.\",\n",
    "        \"terms\": []\n",
    "    },\n",
    "    \"\"\"{\n",
    "        \"name\": \"LumiOpen/Viking-7B\",\n",
    "        \"src_text\": \"When Gemma Lucy Smart received an invitation to attend a conference in the US, she was excited.\",\n",
    "        \"terms\": []\n",
    "    }\"\"\"\n",
    "]\n",
    "\n",
    "# === Load tokenizer and models ===\n",
    "#tokenizer = MarianTokenizer.from_pretrained(models_info[0][\"name\"])\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"LumiOpen/Viking-7B\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "models = []\n",
    "encoder_input_ids_list = []\n",
    "attention_masks_list = []\n",
    "decoder_input_ids_list = []\n",
    "\n",
    "num_beams = 4\n",
    "num_return_sequences = 3\n",
    "max_length = 50\n",
    "eos_token_id = tokenizer.eos_token_id\n",
    "pad_token_id = tokenizer.pad_token_id  # Marian uses pad token to start decoding\n",
    "start_token_id = tokenizer.pad_token_id  # Marian uses pad token to start decoding\n",
    "\n",
    "for info in models_info:\n",
    "    model = MarianMTModel.from_pretrained(info[\"name\"]).to(device).eval()\n",
    "    models.append(model)\n",
    "\n",
    "    # Tokenize and prepare inputs\n",
    "    #enc_inputs = tokenizer(info[\"src_text\"], return_tensors=\"pt\").to(device)\n",
    "    if info[\"terms\"]:\n",
    "        enc_inputs = augment_tokenize(info[\"src_text\"], info[\"terms\"], tokenizer, device)\n",
    "    else:\n",
    "        enc_inputs = tokenizer(info[\"src_text\"], return_tensors=\"pt\").to(device)\n",
    "        \n",
    "    encoder_input_ids = enc_inputs[\"input_ids\"].expand(num_beams, -1).clone()\n",
    "    attention_mask = enc_inputs[\"attention_mask\"].expand(num_beams, -1).clone()\n",
    "\n",
    "    # Init decoder_input_ids with a start token\n",
    "    decoder_input_ids = torch.full(\n",
    "        (num_beams, 1),\n",
    "        fill_value=start_token_id,\n",
    "        dtype=torch.long,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    encoder_input_ids_list.append(encoder_input_ids)\n",
    "    attention_masks_list.append(attention_mask)\n",
    "    decoder_input_ids_list.append(decoder_input_ids)\n",
    "\n",
    "# === Beam search setup ===\n",
    "beam_scorer = BeamSearchScorer(\n",
    "    batch_size=1,\n",
    "    num_beams=num_beams,\n",
    "    device=device,\n",
    "    length_penalty=1.0,\n",
    "    do_early_stopping=True,\n",
    "    num_beam_hyps_to_keep=num_return_sequences,\n",
    ")\n",
    "\n",
    "beam_scores = torch.zeros((num_beams,), dtype=torch.float, device=device)\n",
    "beam_scores[1:] = -1e9  # Only first beam is active at the beginning\n",
    "\n",
    "cur_len = 1  # decoder_input_ids starts with 1 token\n",
    "\n",
    "# === Step-by-step beam search loop ===\n",
    "while cur_len < max_length:\n",
    "    all_log_probs = []\n",
    "\n",
    "    # === Each model provides logits from its own source + decoder input ===\n",
    "    for model, encoder_input_ids, attention_mask, decoder_input_ids in zip(\n",
    "        models, encoder_input_ids_list, attention_masks_list, decoder_input_ids_list\n",
    "    ):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                input_ids=encoder_input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                decoder_input_ids=decoder_input_ids,\n",
    "            )\n",
    "            logits = outputs.logits[:, -1, :]  # (num_beams, vocab_size)\n",
    "            logits[:, [pad_token_id]] = float(\"-inf\")\n",
    "            log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "            all_log_probs.append(log_probs)\n",
    "\n",
    "    # === Shallow fusion: average log_probs ===\n",
    "    avg_log_probs = torch.stack(all_log_probs).mean(dim=0)\n",
    "\n",
    "    # === Beam step ===\n",
    "    next_beam_scores, next_tokens = torch.topk(avg_log_probs, 2, dim=1)\n",
    "    next_beam_scores += beam_scores[:, None]\n",
    "\n",
    "    next_beam_scores = next_beam_scores.view(1, -1)\n",
    "    next_tokens = next_tokens.view(1, -1)\n",
    "    next_indices = torch.arange(num_beams, device=device).repeat_interleave(2).view(1, -1)\n",
    "\n",
    "    decoder_input_ids_for_process = decoder_input_ids_list[0]\n",
    "    if decoder_input_ids_for_process.ndim != 2:\n",
    "        raise ValueError(f\"Expected 2D decoder_input_ids, got shape {decoder_input_ids_for_process.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "    beam_outputs = beam_scorer.process(\n",
    "        decoder_input_ids_for_process,\n",
    "        next_beam_scores,\n",
    "        next_tokens,\n",
    "        next_indices,\n",
    "        eos_token_id=eos_token_id,\n",
    "    )\n",
    "\n",
    "    # === Update decoder_input_ids and beam scores ===\n",
    "    for i in range(len(models)):\n",
    "        decoder_input_ids_list[i] = torch.cat(\n",
    "            [\n",
    "                decoder_input_ids_list[i][beam_outputs.data[\"next_beam_indices\"]],\n",
    "                beam_outputs.data[\"next_beam_tokens\"].unsqueeze(-1),\n",
    "            ],\n",
    "            dim=-1,\n",
    "        )\n",
    "\n",
    "    beam_scores = beam_outputs.data[\"next_beam_scores\"]\n",
    "    cur_len += 1\n",
    "\n",
    "    if beam_scorer.is_done:\n",
    "        break\n",
    "\n",
    "# === Finalize hypotheses ===\n",
    "final_outputs = beam_scorer.finalize(\n",
    "    decoder_input_ids_list[0],\n",
    "    beam_scores,\n",
    "    final_beam_tokens=None,\n",
    "    final_beam_indices=None,\n",
    "    max_length=cur_len,\n",
    "    pad_token_id=pad_token_id,\n",
    "    eos_token_id=eos_token_id\n",
    ")\n",
    "\n",
    "# === Decode translations ===\n",
    "translation = [tokenizer.decode(t, skip_special_tokens=False) for t in final_outputs.data[\"sequences\"][0]]\n",
    "translations = tokenizer.batch_decode(final_outputs.data[\"sequences\"], skip_special_tokens=True,)\n",
    "print(\"\\nðŸŒ Shallow Fusion Translations (Multi-Input):\")\n",
    "print(\"\".join(translations))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
