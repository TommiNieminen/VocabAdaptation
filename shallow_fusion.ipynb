{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46461baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tommi/hf_testing/hf_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import MarianTokenizer, MarianMTModel, AutoModelForCausalLM, AutoTokenizer, StoppingCriteria, StoppingCriteriaList\n",
    "from transformers.generation import BeamSearchScorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1b3b06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "term_model = MarianMTModel.from_pretrained(\"D:/Users/niemi/Documents/Testing/Cuda/termmodel.eng-fin\")\n",
    "term_tokenizer = MarianTokenizer.from_pretrained(\"D:/Users/niemi/Documents/Testing/Cuda/termmodel.eng-fin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5f1872f",
   "metadata": {},
   "outputs": [],
   "source": [
    "viking_vocab_model = MarianMTModel.from_pretrained(\"D:/Users/niemi/Documents/Testing/Cuda/converted-vocab_fix_model_viking7\")\n",
    "viking_tokenizer = AutoTokenizer.from_pretrained(\"LumiOpen/Viking-7B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2a834ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  4.77it/s]\n"
     ]
    }
   ],
   "source": [
    "branch = \"1000B\"\n",
    "viking_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"LumiOpen/Viking-7B\",\n",
    "    torch_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,\n",
    "    revision=branch,\n",
    ").to(\"cuda\")\n",
    "\n",
    "viking_tokenizer = AutoTokenizer.from_pretrained(\"LumiOpen/Viking-7B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3ea4b798",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StoppingCriteriaSub(StoppingCriteria):\n",
    "    def __init__(self,tokenizer, stops = [], encounters=1):\n",
    "        super().__init__()\n",
    "        self.stops = [stop.to(\"cuda\") for stop in stops]\n",
    "        self.tokenizer = tokenizer\n",
    "        self.batch_stop_list = []\n",
    "\n",
    "    def __call__(self, batch_input_ids: torch.LongTensor, scores: torch.FloatTensor):\n",
    "        source_id = 0\n",
    "        for input_ids in batch_input_ids:\n",
    "            if source_id in self.batch_stop_list:\n",
    "                source_id += 1\n",
    "                continue\n",
    "            last_token = input_ids[-1]\n",
    "            for stop in self.stops:\n",
    "                if self.tokenizer.decode(stop) in self.tokenizer.decode(last_token):\n",
    "                    print(\"stop:\",self.tokenizer.decode(stop))\n",
    "                    print(\"last token:\",self.tokenizer.decode(last_token))\n",
    "                    self.batch_stop_list.append(source_id)\n",
    "            source_id += 1\n",
    "        if len(self.batch_stop_list) == len(batch_input_ids):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "# Stop at line break\n",
    "stop_words = [\"<|im_end|>\"]\n",
    "stop_words_ids = [viking_tokenizer(stop_word, return_tensors='pt', add_special_tokens=False)['input_ids'].squeeze() for stop_word in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "69e64c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stop: <|im_end|>\n",
      "last token: <|im_end|>\n",
      "<|im_start|>user\n",
      "Translate into Finnish: The Sheilasâ€™ Wheels owner, Esure, will be sold to the Belgian insurer Ageas.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Sheilas' Wheelsin omistaja Esure myydÃ¤Ã¤n belgialaiselle vakuutusyhtiÃ¶lle Ageasille.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "stopping_criteria = StoppingCriteriaList([StoppingCriteriaSub(viking_tokenizer, stops=stop_words_ids)])\n",
    "inputs = viking_tokenizer(\"<|im_start|>user\\nTranslate into Finnish: The Sheilasâ€™ Wheels owner, Esure, will be sold to the Belgian insurer Ageas.<|im_end|>\\n<|im_start|>assistant\\n\",return_tensors=\"pt\").to(\"cuda\")\n",
    "translated = viking_model.generate(**inputs,max_length=200, stopping_criteria=stopping_criteria)\n",
    "print(viking_tokenizer.decode(translated[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "98b34259",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'term_tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mterm_tokenizer\u001b[49m.get_vocab()[\u001b[33m\"\u001b[39m\u001b[33maugmentsymbol1\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[31mNameError\u001b[39m: name 'term_tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "print(term_tokenizer.get_vocab()[\"augmentsymbol1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ff9ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def augment_tokenize(text, terms, tokenizer, device=\"cpu\"):\n",
    "    vocab = tokenizer.get_vocab()\n",
    "    text_tokenized = tokenizer(text).input_ids\n",
    "    for term_source,term_target in terms:\n",
    "        term_source_tokenized = tokenizer(term_source).input_ids[:-1]\n",
    "        term_target_tokenized = list(tokenizer(text_target=term_target).input_ids)[:-1]\n",
    "        term_target_tokenized = [vocab[\"augmentsymbol1\"]] + \\\n",
    "            term_target_tokenized + \\\n",
    "            [vocab[\"augmentsymbol2\"]]\n",
    "            \n",
    "        current_aug_part_index = 0\n",
    "        new_text_tokenized = []\n",
    "        for token in text_tokenized:\n",
    "            #TODO: add check for the word continuing\n",
    "            if current_aug_part_index == len(term_source_tokenized):\n",
    "                new_text_tokenized += [vocab[\"augmentsymbol0\"]] + term_source_tokenized + \\\n",
    "                term_target_tokenized\n",
    "                current_aug_part_index = 0\n",
    "            if token == term_source_tokenized[current_aug_part_index]:\n",
    "                current_aug_part_index += 1\n",
    "            elif current_aug_part_index > 1:\n",
    "                new_text_tokenized += term_source_tokenized[0:current_aug_part_index]\n",
    "                new_text_tokenized.append(token)\n",
    "                current_aug_part_index = 0\n",
    "            else:\n",
    "                new_text_tokenized.append(token)\n",
    "        text_tokenized = new_text_tokenized\n",
    "\n",
    "    input_ids = torch.tensor([text_tokenized], device=device)  # batch dimension added\n",
    "    attention_mask = torch.ones_like(input_ids, device=device)\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41be608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[57829, 57829, 48685, 57440, 45603, 57432, 34377, 57291, 36046, 48683,\n",
      "         57440, 48558, 57432, 24547, 57291,    46, 41756]])\n",
      "<pad> <pad> Theaugmentsymbol0 studentaugmentsymbol1 oppilasaugmentsymbol2 passed theaugmentsymbol0 testaugmentsymbol1 koeaugmentsymbol2.</s>\n",
      "{'input_ids': tensor([[48685,  6090, 16089, 30124, 55986, 28752,  7250,   275, 45603,  6126,\n",
      "         41755, 55986, 28752,  7250,   446, 34235, 36996,  5567, 51683, 30124,\n",
      "         55986, 28752,  7250,  1177, 36046, 48683, 48558,    46, 41756]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1]])}\n",
      "Testin lÃ¤pÃ¤isi auppsymbol1 oppilaseugmentsymbol2 lÃ¤pÃ¤istyÃ¤Ã¤n tutkimuksensa, jonka tulokset olivat samat kuin muidenkin opiskelijoiden ja opiskelijoiden, joiden tutkimustulokset olivat samat kuin opiskelijoiden ja opiskelijoiden, joiden tutkimustulokset olivat samat kuin opiskelijoiden ja opiskelijoiden, joiden tutkimustulokset olivat samat kuin opiskelijoiden ja opiskelijoiden, joiden tutkimustulokset olivat samat kuin muiden opiskelijoiden, ja jotka olivat samat kuin opiskelijoiden ja opiskelijoiden, joiden tutkimustulokset olivat samat kuin muiden opiskelijoiden, ja jotka olivat\n"
     ]
    }
   ],
   "source": [
    "terms = [(\"student\",\"oppilas\"),(\"test\",\"koe\")]\n",
    "inputs = augment_tokenize(\"The student passed the test.\", terms, term_tokenizer)\n",
    "print(inputs[\"input_ids\"])\n",
    "print(term_tokenizer.decode(inputs[\"input_ids\"][0]))\n",
    "inputs1 = term_tokenizer(\"The student passed the test.\",return_tensors=\"pt\")\n",
    "print(inputs1)\n",
    "# Generate translation with max length\n",
    "translated = term_model.generate(**inputs, max_length=100, num_beams=12)\n",
    "\n",
    "# Decode the translation\n",
    "translated_text = term_tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "print(translated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8e1c460",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tommi/hf_testing/hf_env/lib/python3.9/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for MarianMTModel:\n\tsize mismatch for final_logits_bias: copying a param with shape torch.Size([1, 57829]) from checkpoint, the shape in current model is torch.Size([1, 57830]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m start_token_id \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mpad_token_id  \u001b[38;5;66;03m# Marian uses pad token to start decoding\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m info \u001b[38;5;129;01min\u001b[39;00m models_info:\n\u001b[0;32m---> 35\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mMarianMTModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43minfo\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     36\u001b[0m     models\u001b[38;5;241m.\u001b[39mappend(model)\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;66;03m# Tokenize and prepare inputs\u001b[39;00m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m#enc_inputs = tokenizer(info[\"src_text\"], return_tensors=\"pt\").to(device)\u001b[39;00m\n",
      "File \u001b[0;32m~/hf_testing/hf_env/lib/python3.9/site-packages/transformers/modeling_utils.py:279\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 279\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    281\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[0;32m~/hf_testing/hf_env/lib/python3.9/site-packages/transformers/modeling_utils.py:4399\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4389\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4390\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   4392\u001b[0m     (\n\u001b[1;32m   4393\u001b[0m         model,\n\u001b[1;32m   4394\u001b[0m         missing_keys,\n\u001b[1;32m   4395\u001b[0m         unexpected_keys,\n\u001b[1;32m   4396\u001b[0m         mismatched_keys,\n\u001b[1;32m   4397\u001b[0m         offload_index,\n\u001b[1;32m   4398\u001b[0m         error_msgs,\n\u001b[0;32m-> 4399\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4401\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4402\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4405\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4408\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4415\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4417\u001b[0m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[1;32m   4418\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m~/hf_testing/hf_env/lib/python3.9/site-packages/transformers/modeling_utils.py:4833\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001b[0m\n\u001b[1;32m   4831\u001b[0m \u001b[38;5;66;03m# Skip it with fsdp on ranks other than 0\u001b[39;00m\n\u001b[1;32m   4832\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_fsdp_enabled() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_local_dist_rank_0() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_quantized):\n\u001b[0;32m-> 4833\u001b[0m     disk_offload_index, cpu_offload_index \u001b[38;5;241m=\u001b[39m \u001b[43m_load_state_dict_into_meta_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4834\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4835\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4836\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshard_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4837\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4838\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreverse_key_renaming_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4839\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4840\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4841\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisk_offload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4842\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcpu_offload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcpu_offload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4843\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcpu_offload_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcpu_offload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4844\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4845\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_offloaded_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4846\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4847\u001b[0m \u001b[43m        \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4848\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4849\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4851\u001b[0m \u001b[38;5;66;03m# force memory release if loading multiple shards, to avoid having 2 state dicts in memory in next loop\u001b[39;00m\n\u001b[1;32m   4852\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m state_dict\n",
      "File \u001b[0;32m~/hf_testing/hf_env/lib/python3.9/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/hf_testing/hf_env/lib/python3.9/site-packages/transformers/modeling_utils.py:824\u001b[0m, in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, shard_file, expected_keys, reverse_renaming_mapping, device_map, disk_offload_folder, disk_offload_index, cpu_offload_folder, cpu_offload_index, hf_quantizer, is_safetensors, keep_in_fp32_regex, unexpected_keys, device_mesh)\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_fsdp_enabled():\n\u001b[1;32m    822\u001b[0m         param_device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_local_dist_rank_0() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 824\u001b[0m     \u001b[43m_load_parameter_into_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam_device\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     hf_quantizer\u001b[38;5;241m.\u001b[39mcreate_quantized_param(\n\u001b[1;32m    828\u001b[0m         model, param, param_name, param_device, state_dict, unexpected_keys\n\u001b[1;32m    829\u001b[0m     )\n",
      "File \u001b[0;32m~/hf_testing/hf_env/lib/python3.9/site-packages/transformers/modeling_utils.py:712\u001b[0m, in \u001b[0;36m_load_parameter_into_model\u001b[0;34m(model, param_name, tensor)\u001b[0m\n\u001b[1;32m    710\u001b[0m module, param_type \u001b[38;5;241m=\u001b[39m get_module_from_name(model, param_name)\n\u001b[1;32m    711\u001b[0m \u001b[38;5;66;03m# This will check potential shape mismatch if skipped before\u001b[39;00m\n\u001b[0;32m--> 712\u001b[0m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[43mparam_type\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43massign\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/hf_testing/hf_env/lib/python3.9/site-packages/torch/nn/modules/module.py:2581\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2573\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2574\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   2575\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2576\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[1;32m   2577\u001b[0m             ),\n\u001b[1;32m   2578\u001b[0m         )\n\u001b[1;32m   2580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2581\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   2582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2583\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[1;32m   2584\u001b[0m         )\n\u001b[1;32m   2585\u001b[0m     )\n\u001b[1;32m   2586\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for MarianMTModel:\n\tsize mismatch for final_logits_bias: copying a param with shape torch.Size([1, 57829]) from checkpoint, the shape in current model is torch.Size([1, 57830])."
     ]
    }
   ],
   "source": [
    "# === Define models and their unique source inputs ===\n",
    "models_info = [\n",
    "    \n",
    "    {\n",
    "        \"name\": \"/home/tommi/hf_testing/converted-testmodel\",\n",
    "        \"src_text\": \"This is a test.\",\n",
    "        \"terms\": []\n",
    "    },\n",
    "]\n",
    "\n",
    "\"\"\"{\n",
    "    \"name\": \"LumiOpen/Viking-7B\",\n",
    "    \"src_text\": \"When Gemma Lucy Smart received an invitation to attend a conference in the US, she was excited.\",\n",
    "    \"terms\": []\n",
    "}\"\"\"\n",
    "\n",
    "# === Load tokenizer and models ===\n",
    "tokenizer = MarianTokenizer.from_pretrained(models_info[0][\"name\"])\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"LumiOpen/Viking-7B\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "models = []\n",
    "encoder_input_ids_list = []\n",
    "attention_masks_list = []\n",
    "decoder_input_ids_list = []\n",
    "\n",
    "num_beams = 4\n",
    "num_return_sequences = 3\n",
    "max_length = 50\n",
    "eos_token_id = tokenizer.eos_token_id\n",
    "pad_token_id = tokenizer.pad_token_id  # Marian uses pad token to start decoding\n",
    "start_token_id = tokenizer.pad_token_id  # Marian uses pad token to start decoding\n",
    "\n",
    "for info in models_info:\n",
    "    model = MarianMTModel.from_pretrained(info[\"name\"]).to(device).eval()\n",
    "    models.append(model)\n",
    "\n",
    "    # Tokenize and prepare inputs\n",
    "    #enc_inputs = tokenizer(info[\"src_text\"], return_tensors=\"pt\").to(device)\n",
    "    if info[\"terms\"]:\n",
    "        enc_inputs = augment_tokenize(info[\"src_text\"], info[\"terms\"], tokenizer, device)\n",
    "    else:\n",
    "        enc_inputs = tokenizer(info[\"src_text\"], return_tensors=\"pt\").to(device)\n",
    "        \n",
    "    encoder_input_ids = enc_inputs[\"input_ids\"].expand(num_beams, -1).clone()\n",
    "    attention_mask = enc_inputs[\"attention_mask\"].expand(num_beams, -1).clone()\n",
    "\n",
    "    # Init decoder_input_ids with a start token\n",
    "    decoder_input_ids = torch.full(\n",
    "        (num_beams, 1),\n",
    "        fill_value=start_token_id,\n",
    "        dtype=torch.long,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    encoder_input_ids_list.append(encoder_input_ids)\n",
    "    attention_masks_list.append(attention_mask)\n",
    "    decoder_input_ids_list.append(decoder_input_ids)\n",
    "\n",
    "# === Beam search setup ===\n",
    "beam_scorer = BeamSearchScorer(\n",
    "    batch_size=1,\n",
    "    num_beams=num_beams,\n",
    "    device=device,\n",
    "    length_penalty=1.0,\n",
    "    do_early_stopping=True,\n",
    "    num_beam_hyps_to_keep=num_return_sequences,\n",
    ")\n",
    "\n",
    "beam_scores = torch.zeros((num_beams,), dtype=torch.float, device=device)\n",
    "beam_scores[1:] = -1e9  # Only first beam is active at the beginning\n",
    "\n",
    "cur_len = 1  # decoder_input_ids starts with 1 token\n",
    "\n",
    "# === Step-by-step beam search loop ===\n",
    "while cur_len < max_length:\n",
    "    all_log_probs = []\n",
    "\n",
    "    # === Each model provides logits from its own source + decoder input ===\n",
    "    for model, encoder_input_ids, attention_mask, decoder_input_ids in zip(\n",
    "        models, encoder_input_ids_list, attention_masks_list, decoder_input_ids_list\n",
    "    ):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                input_ids=encoder_input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                decoder_input_ids=decoder_input_ids,\n",
    "            )\n",
    "            logits = outputs.logits[:, -1, :]  # (num_beams, vocab_size)\n",
    "            logits[:, [pad_token_id]] = float(\"-inf\")\n",
    "            log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "            all_log_probs.append(log_probs)\n",
    "\n",
    "    # === Shallow fusion: average log_probs ===\n",
    "    avg_log_probs = torch.stack(all_log_probs).mean(dim=0)\n",
    "\n",
    "    # === Beam step ===\n",
    "    next_beam_scores, next_tokens = torch.topk(avg_log_probs, 2, dim=1)\n",
    "    next_beam_scores += beam_scores[:, None]\n",
    "\n",
    "    next_beam_scores = next_beam_scores.view(1, -1)\n",
    "    next_tokens = next_tokens.view(1, -1)\n",
    "    print([tokenizer.decode(x) for x in next_tokens])\n",
    "    next_indices = torch.arange(num_beams, device=device).repeat_interleave(2).view(1, -1)\n",
    "\n",
    "    decoder_input_ids_for_process = decoder_input_ids_list[0]\n",
    "    if decoder_input_ids_for_process.ndim != 2:\n",
    "        raise ValueError(f\"Expected 2D decoder_input_ids, got shape {decoder_input_ids_for_process.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "    beam_outputs = beam_scorer.process(\n",
    "        decoder_input_ids_for_process,\n",
    "        next_beam_scores,\n",
    "        next_tokens,\n",
    "        next_indices,\n",
    "        eos_token_id=eos_token_id,\n",
    "    )\n",
    "\n",
    "    # === Update decoder_input_ids and beam scores ===\n",
    "    for i in range(len(models)):\n",
    "        decoder_input_ids_list[i] = torch.cat(\n",
    "            [\n",
    "                decoder_input_ids_list[i][beam_outputs.data[\"next_beam_indices\"]],\n",
    "                beam_outputs.data[\"next_beam_tokens\"].unsqueeze(-1),\n",
    "            ],\n",
    "            dim=-1,\n",
    "        )\n",
    "\n",
    "    beam_scores = beam_outputs.data[\"next_beam_scores\"]\n",
    "    cur_len += 1\n",
    "\n",
    "    if beam_scorer.is_done:\n",
    "        break\n",
    "\n",
    "# === Finalize hypotheses ===\n",
    "final_outputs = beam_scorer.finalize(\n",
    "    decoder_input_ids_list[0],\n",
    "    beam_scores,\n",
    "    final_beam_tokens=None,\n",
    "    final_beam_indices=None,\n",
    "    max_length=cur_len,\n",
    "    pad_token_id=pad_token_id,\n",
    "    eos_token_id=eos_token_id\n",
    ")\n",
    "\n",
    "# === Decode translations ===\n",
    "translation = [tokenizer.decode(t, skip_special_tokens=False) for t in final_outputs.data[\"sequences\"][0]]\n",
    "translations = tokenizer.batch_decode(final_outputs.data[\"sequences\"], skip_special_tokens=True,)\n",
    "print(\"\\nðŸŒ Shallow Fusion Translations (Multi-Input):\")\n",
    "print(\"\".join(translations))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da0c787a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " TÃ¤mÃ¤ on kova testi."
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"LumiOpen/Viking-7B\")\n",
    "#tokenizer = MarianTokenizer.from_pretrained(\"/home/tommi/hf_testing/converted-vocab_fix_model\")\n",
    "model = MarianMTModel.from_pretrained(\"/home/tommi/hf_testing/converted-vocab_fix_model\").to(\"cpu\")\n",
    "input_ids = tokenizer(\"This is a hard test.</s>\",return_tensors=\"pt\").input_ids.to(\"cpu\")\n",
    "decoder_input_ids = tokenizer(\"<pad><s>\",return_tensors=\"pt\").input_ids.to(\"cpu\")\n",
    "for _ in range(100):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "        input_ids=input_ids,\n",
    "        decoder_input_ids=decoder_input_ids,\n",
    "    )\n",
    "        logits = outputs.logits\n",
    "        next_token_logits = logits[:, -1, :]\n",
    "        next_token_logits[0][tokenizer.pad_token_id] = float(\"-inf\")\n",
    "        next_token_id = torch.argmax(next_token_logits, dim=-1).unsqueeze(0)\n",
    "        decoder_input_ids = torch.cat([decoder_input_ids, next_token_id], dim=-1)\n",
    "        if next_token_id[0][0] == tokenizer.eos_token_id:\n",
    "            break\n",
    "        new_token = tokenizer.decode(next_token_id.squeeze(), skip_special_tokens=True)\n",
    "        print(new_token, end=\"\", flush=True)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
