{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46461baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\niemi\\Documents\\Testing\\Cuda\\hf_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import MarianTokenizer, MarianMTModel, AutoModelForCausalLM, AutoTokenizer, StoppingCriteria, StoppingCriteriaList, AutoConfig\n",
    "from transformers.generation import BeamSearchScorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1b3b06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "term_model = MarianMTModel.from_pretrained(\"D:/Users/niemi/Documents/Testing/Cuda/termmodel.eng-fin\")\n",
    "term_tokenizer = MarianTokenizer.from_pretrained(\"D:/Users/niemi/Documents/Testing/Cuda/termmodel.eng-fin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5f1872f",
   "metadata": {},
   "outputs": [],
   "source": [
    "viking_vocab_model = MarianMTModel.from_pretrained(\"D:/Users/niemi/Documents/Testing/Cuda/converted-vocab_fix_model_viking7\")\n",
    "viking_tokenizer = AutoTokenizer.from_pretrained(\"LumiOpen/Viking-7B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2a834ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00,  4.77it/s]\n"
     ]
    }
   ],
   "source": [
    "branch = \"1000B\"\n",
    "viking_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"LumiOpen/Viking-7B\",\n",
    "    torch_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,\n",
    "    revision=branch,\n",
    ").to(\"cuda\")\n",
    "\n",
    "viking_tokenizer = AutoTokenizer.from_pretrained(\"LumiOpen/Viking-7B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3ea4b798",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StoppingCriteriaSub(StoppingCriteria):\n",
    "    def __init__(self,tokenizer, stops = [], encounters=1):\n",
    "        super().__init__()\n",
    "        self.stops = [stop.to(\"cuda\") for stop in stops]\n",
    "        self.tokenizer = tokenizer\n",
    "        self.batch_stop_list = []\n",
    "\n",
    "    def __call__(self, batch_input_ids: torch.LongTensor, scores: torch.FloatTensor):\n",
    "        source_id = 0\n",
    "        for input_ids in batch_input_ids:\n",
    "            if source_id in self.batch_stop_list:\n",
    "                source_id += 1\n",
    "                continue\n",
    "            last_token = input_ids[-1]\n",
    "            for stop in self.stops:\n",
    "                if self.tokenizer.decode(stop) in self.tokenizer.decode(last_token):\n",
    "                    print(\"stop:\",self.tokenizer.decode(stop))\n",
    "                    print(\"last token:\",self.tokenizer.decode(last_token))\n",
    "                    self.batch_stop_list.append(source_id)\n",
    "            source_id += 1\n",
    "        if len(self.batch_stop_list) == len(batch_input_ids):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "# Stop at line break\n",
    "stop_words = [\"<|im_end|>\"]\n",
    "stop_words_ids = [viking_tokenizer(stop_word, return_tensors='pt', add_special_tokens=False)['input_ids'].squeeze() for stop_word in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "69e64c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stop: <|im_end|>\n",
      "last token: <|im_end|>\n",
      "<|im_start|>user\n",
      "Translate into Finnish: The Sheilas‚Äô Wheels owner, Esure, will be sold to the Belgian insurer Ageas.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Sheilas' Wheelsin omistaja Esure myyd√§√§n belgialaiselle vakuutusyhti√∂lle Ageasille.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "stopping_criteria = StoppingCriteriaList([StoppingCriteriaSub(viking_tokenizer, stops=stop_words_ids)])\n",
    "inputs = viking_tokenizer(\"<|im_start|>user\\nTranslate into Finnish: The Sheilas‚Äô Wheels owner, Esure, will be sold to the Belgian insurer Ageas.<|im_end|>\\n<|im_start|>assistant\\n\",return_tensors=\"pt\").to(\"cuda\")\n",
    "translated = viking_model.generate(**inputs,max_length=200, stopping_criteria=stopping_criteria)\n",
    "print(viking_tokenizer.decode(translated[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "98b34259",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'term_tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mterm_tokenizer\u001b[49m.get_vocab()[\u001b[33m\"\u001b[39m\u001b[33maugmentsymbol1\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[31mNameError\u001b[39m: name 'term_tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "print(term_tokenizer.get_vocab()[\"augmentsymbol1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ff9ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def augment_tokenize(text, terms, tokenizer, device=\"cpu\"):\n",
    "    vocab = tokenizer.get_vocab()\n",
    "    text_tokenized = tokenizer(text).input_ids\n",
    "    for term_source,term_target in terms:\n",
    "        term_source_tokenized = tokenizer(term_source).input_ids[:-1]\n",
    "        term_target_tokenized = list(tokenizer(text_target=term_target).input_ids)[:-1]\n",
    "        term_target_tokenized = [vocab[\"augmentsymbol1\"]] + \\\n",
    "            term_target_tokenized + \\\n",
    "            [vocab[\"augmentsymbol2\"]]\n",
    "            \n",
    "        current_aug_part_index = 0\n",
    "        new_text_tokenized = []\n",
    "        for token in text_tokenized:\n",
    "            #TODO: add check for the word continuing\n",
    "            if current_aug_part_index == len(term_source_tokenized):\n",
    "                new_text_tokenized += [vocab[\"augmentsymbol0\"]] + term_source_tokenized + \\\n",
    "                term_target_tokenized\n",
    "                current_aug_part_index = 0\n",
    "            if token == term_source_tokenized[current_aug_part_index]:\n",
    "                current_aug_part_index += 1\n",
    "            elif current_aug_part_index > 1:\n",
    "                new_text_tokenized += term_source_tokenized[0:current_aug_part_index]\n",
    "                new_text_tokenized.append(token)\n",
    "                current_aug_part_index = 0\n",
    "            else:\n",
    "                new_text_tokenized.append(token)\n",
    "        text_tokenized = new_text_tokenized\n",
    "\n",
    "    input_ids = torch.tensor([text_tokenized], device=device)  # batch dimension added\n",
    "    attention_mask = torch.ones_like(input_ids, device=device)\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41be608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[57829, 57829, 48685, 57440, 45603, 57432, 34377, 57291, 36046, 48683,\n",
      "         57440, 48558, 57432, 24547, 57291,    46, 41756]])\n",
      "<pad> <pad> Theaugmentsymbol0 studentaugmentsymbol1 oppilasaugmentsymbol2 passed theaugmentsymbol0 testaugmentsymbol1 koeaugmentsymbol2.</s>\n",
      "{'input_ids': tensor([[48685,  6090, 16089, 30124, 55986, 28752,  7250,   275, 45603,  6126,\n",
      "         41755, 55986, 28752,  7250,   446, 34235, 36996,  5567, 51683, 30124,\n",
      "         55986, 28752,  7250,  1177, 36046, 48683, 48558,    46, 41756]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1]])}\n",
      "Testin l√§p√§isi auppsymbol1 oppilaseugmentsymbol2 l√§p√§isty√§√§n tutkimuksensa, jonka tulokset olivat samat kuin muidenkin opiskelijoiden ja opiskelijoiden, joiden tutkimustulokset olivat samat kuin opiskelijoiden ja opiskelijoiden, joiden tutkimustulokset olivat samat kuin opiskelijoiden ja opiskelijoiden, joiden tutkimustulokset olivat samat kuin opiskelijoiden ja opiskelijoiden, joiden tutkimustulokset olivat samat kuin muiden opiskelijoiden, ja jotka olivat samat kuin opiskelijoiden ja opiskelijoiden, joiden tutkimustulokset olivat samat kuin muiden opiskelijoiden, ja jotka olivat\n"
     ]
    }
   ],
   "source": [
    "terms = [(\"student\",\"oppilas\"),(\"test\",\"koe\")]\n",
    "inputs = augment_tokenize(\"The student passed the test.\", terms, term_tokenizer)\n",
    "print(inputs[\"input_ids\"])\n",
    "print(term_tokenizer.decode(inputs[\"input_ids\"][0]))\n",
    "inputs1 = term_tokenizer(\"The student passed the test.\",return_tensors=\"pt\")\n",
    "print(inputs1)\n",
    "# Generate translation with max length\n",
    "translated = term_model.generate(**inputs, max_length=100, num_beams=12)\n",
    "\n",
    "# Decode the translation\n",
    "translated_text = term_tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "print(translated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4aabf724",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name_or_path, device):\n",
    "    config = AutoConfig.from_pretrained(model_name_or_path)\n",
    "\n",
    "    # Check model type by architecture string\n",
    "    if \"Marian\" in config.model_type or config.architectures and any(\"Marian\" in arch for arch in config.architectures):\n",
    "        model = MarianMTModel.from_pretrained(model_name_or_path).to(device).eval()\n",
    "        model_type = \"marian\"\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name_or_path,\n",
    "            torch_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16).to(device).eval()\n",
    "        model_type = \"causal_lm\"\n",
    "\n",
    "    return model, model_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4a311df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenizer(model_name_or_path):\n",
    "    config = AutoConfig.from_pretrained(model_name_or_path)\n",
    "\n",
    "    # Check model type by architecture string\n",
    "    if \"Marian\" in config.model_type or config.architectures and any(\"Marian\" in arch for arch in config.architectures):\n",
    "        tokenizer = MarianTokenizer.from_pretrained(model_name_or_path)\n",
    "    else:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d433a60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShallowFusion():\n",
    "    def __init__(self, models_info):\n",
    "        self.models_info = models_info\n",
    "        # === Load tokenizer and models ===\n",
    "        # The tokenizer of the first model is used\n",
    "        self.tokenizer = load_tokenizer(models_info[0][\"name\"])\n",
    "        \n",
    "        # tokenizer = AutoTokenizer.from_pretrained(\"LumiOpen/Viking-7B\")\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.init_models()\n",
    "\n",
    "    def init_models(self):\n",
    "        self.models = []        \n",
    "        for info in self.models_info:\n",
    "            model, model_type = load_model(info[\"name\"], self.device)\n",
    "            self.models.append((model,model_type))\n",
    "\n",
    "    def initialize_inputs(self, src_sentences_per_model):\n",
    "        self.encoder_input_ids_list = []\n",
    "        self.attention_masks_list = []\n",
    "        self.decoder_input_ids_list = []\n",
    "\n",
    "        num_beams = 4\n",
    "        self.eos_token_id = self.tokenizer.eos_token_id\n",
    "        self.pad_token_id = self.tokenizer.pad_token_id  # Marian uses pad token to start decoding\n",
    "        self.start_token_id = self.tokenizer.pad_token_id  # Marian uses pad token to start decoding\n",
    "\n",
    "        for (sentence,terms),(model,model_type) in zip(src_sentences_per_model,self.models):\n",
    "            # Tokenize and prepare inputs\n",
    "            if terms:\n",
    "                enc_inputs = augment_tokenize(sentence, terms, self.tokenizer, self.device)\n",
    "            else:\n",
    "                enc_inputs = self.tokenizer(sentence, return_tensors=\"pt\").to(self.device)\n",
    "                \n",
    "            encoder_input_ids = enc_inputs[\"input_ids\"].expand(num_beams, -1).clone()\n",
    "            attention_mask = enc_inputs[\"attention_mask\"].expand(num_beams, -1).clone()\n",
    "\n",
    "            self.encoder_input_ids_list.append(encoder_input_ids)\n",
    "            self.attention_masks_list.append(attention_mask)\n",
    "\n",
    "            if model_type == \"marian\":\n",
    "                # Init decoder_input_ids with a start token\n",
    "                decoder_input_ids = torch.full(\n",
    "                    (num_beams, 1),\n",
    "                    fill_value=self.start_token_id,\n",
    "                    dtype=torch.long,\n",
    "                    device=self.device,\n",
    "                )\n",
    "            else:\n",
    "                # LLMs keep track of decoder input ids, but concat them with the input for generation\n",
    "                decoder_input_ids = torch.empty((num_beams, 0), dtype=torch.long, device=self.device)\n",
    "            self.decoder_input_ids_list.append(decoder_input_ids)\n",
    "    \n",
    "    def translate(self, src_sentences_per_model, num_beams=4, max_length=50):\n",
    "        self.initialize_inputs(src_sentences_per_model)\n",
    "        \n",
    "        # === Beam search setup ===\n",
    "        beam_scorer = BeamSearchScorer(\n",
    "            batch_size=1,\n",
    "            num_beams=num_beams,\n",
    "            device=self.device,\n",
    "            length_penalty=1.0,\n",
    "            do_early_stopping=True,\n",
    "            num_beam_hyps_to_keep=num_beams,\n",
    "        )\n",
    "\n",
    "        beam_scores = torch.zeros((num_beams,), dtype=torch.float, device=self.device)\n",
    "        beam_scores[1:] = -1e9  # Only first beam is active at the beginning\n",
    "\n",
    "        cur_len = 1  # decoder_input_ids starts with 1 token\n",
    "\n",
    "        # === Step-by-step beam search loop ===\n",
    "        while cur_len < max_length:\n",
    "            all_log_probs = []\n",
    "\n",
    "            # === Each model provides logits from its own source + decoder input ===\n",
    "            for (model,model_type), encoder_input_ids, attention_mask, decoder_input_ids in zip(\n",
    "                self.models, self.encoder_input_ids_list, self.attention_masks_list, self.decoder_input_ids_list\n",
    "            ):\n",
    "                with torch.no_grad():\n",
    "                    if model_type == \"marian\":\n",
    "                        outputs = model(\n",
    "                            input_ids=encoder_input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            decoder_input_ids=decoder_input_ids,\n",
    "                        )\n",
    "                        logits = outputs.logits[:, -1, :]  # (num_beams, vocab_size)\n",
    "                        logits[:, [self.pad_token_id]] = float(\"-inf\")\n",
    "                    else:\n",
    "                        input_ids = torch.cat([encoder_input_ids, decoder_input_ids], dim=-1)\n",
    "                        outputs = model(\n",
    "                            input_ids=input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                        )\n",
    "                        logits = outputs.logits[:, -1, :]  # (num_beams, vocab_size)\n",
    "\n",
    "                    log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "                    all_log_probs.append(log_probs)\n",
    "\n",
    "            # === Shallow fusion: average log_probs ===\n",
    "            avg_log_probs = torch.stack(all_log_probs).mean(dim=0)\n",
    "\n",
    "            # === Beam step ===\n",
    "            next_beam_scores, next_tokens = torch.topk(avg_log_probs, 2, dim=1)\n",
    "            next_beam_scores += beam_scores[:, None]\n",
    "\n",
    "            next_beam_scores = next_beam_scores.view(1, -1)\n",
    "            next_tokens = next_tokens.view(1, -1)\n",
    "            next_indices = torch.arange(num_beams, device=self.device).repeat_interleave(2).view(1, -1)\n",
    "\n",
    "            decoder_input_ids_for_process = self.decoder_input_ids_list[0]\n",
    "            \n",
    "            beam_outputs = beam_scorer.process(\n",
    "                decoder_input_ids_for_process,\n",
    "                next_beam_scores,\n",
    "                next_tokens,\n",
    "                next_indices,\n",
    "                eos_token_id=self.eos_token_id,\n",
    "            )\n",
    "\n",
    "            # === Update decoder_input_ids and beam scores ===\n",
    "            for i in range(len(self.models)):\n",
    "                self.decoder_input_ids_list[i] = torch.cat(\n",
    "                    [\n",
    "                        self.decoder_input_ids_list[i][beam_outputs.data[\"next_beam_indices\"]],\n",
    "                        beam_outputs.data[\"next_beam_tokens\"].unsqueeze(-1),\n",
    "                    ],\n",
    "                    dim=-1,\n",
    "                )\n",
    "\n",
    "            beam_scores = beam_outputs.data[\"next_beam_scores\"]\n",
    "            cur_len += 1\n",
    "\n",
    "            if beam_scorer.is_done:\n",
    "                break\n",
    "\n",
    "        # === Finalize hypotheses ===\n",
    "        final_outputs = beam_scorer.finalize(\n",
    "            self.decoder_input_ids_list[0],\n",
    "            beam_scores,\n",
    "            final_beam_tokens=None,\n",
    "            final_beam_indices=None,\n",
    "            max_length=cur_len,\n",
    "            pad_token_id=self.pad_token_id,\n",
    "            eos_token_id=self.eos_token_id\n",
    "        )\n",
    "\n",
    "        # === Decode translations ===\n",
    "        translation = [self.tokenizer.decode(t, skip_special_tokens=False) for t in final_outputs.data[\"sequences\"][0]]\n",
    "        translations = self.tokenizer.batch_decode(final_outputs.data[\"sequences\"], skip_special_tokens=True,)\n",
    "        return translations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c047a1ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ShallowFusion' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# === Define models and their unique source inputs ===\u001b[39;00m\n\u001b[32m      2\u001b[39m models_info = [\n\u001b[32m      3\u001b[39m     {\n\u001b[32m      4\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mLumiOpen/Viking-7B\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m      8\u001b[39m     },\n\u001b[32m      9\u001b[39m ]\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m shallow_fusion = \u001b[43mShallowFusion\u001b[49m(models_info)\n",
      "\u001b[31mNameError\u001b[39m: name 'ShallowFusion' is not defined"
     ]
    }
   ],
   "source": [
    "# === Define models and their unique source inputs ===\n",
    "models_info = [\n",
    "    {\n",
    "        \"name\": \"LumiOpen/Viking-7B\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"../converted-vocab_fix_model_viking7\",\n",
    "    },\n",
    "]\n",
    "\n",
    "shallow_fusion = ShallowFusion(models_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44b8ed08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viking_template(sentence):\n",
    "    return (f\"<|im_start|>user\\nTranslate into Finnish: {sentence}.<|im_end|>\\n<|im_start|>assistant\\n\",[])\n",
    "\n",
    "def marian_llmvoc_template(sentence):\n",
    "    return (sentence + \"</s>\", [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d0f0935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<|im_start|>user\\nTranslate into Finnish:  The colourless and odourless gas is used to create an atmosphere of extreme subzero temperatures..<|im_end|>\\n<|im_start|>assistant\\n', []), (' The colourless and odourless gas is used to create an atmosphere of extreme subzero temperatures.</s>', [])]\n",
      "\n",
      "üåç Shallow Fusion Translations (Multi-Input):\n",
      "V√§rit√∂n ja hajuton kaasu on k√§ytetty luomaan √§√§rimm√§isen pakkasilman ilmapiiri.\n",
      "V√§rit√∂n ja hajuton kaasu on k√§ytetty luomaan √§√§rimm√§isen pakkasilman.\n",
      "V√§rit√∂n ja hajuton kaasu on k√§ytetty luomaan √§√§rimm√§isen pakkasilman.\n",
      "\n",
      "V√§rit√∂n ja hajuton kaasu on k√§ytetty luomaan √§√§rimm√§isen pakkasilman ilmapiiri.\n"
     ]
    }
   ],
   "source": [
    "test_sent = \" The colourless and odourless gas is used to create an atmosphere of extreme subzero temperatures.\"\n",
    "input_sentences = [viking_template(test_sent), marian_llmvoc_template(test_sent)]\n",
    "print(input_sentences)\n",
    "translations = shallow_fusion.translate(input_sentences)\n",
    "\n",
    "print(\"\\nüåç Shallow Fusion Translations (Multi-Input):\")\n",
    "print(\"\\n\".join(translations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0c787a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " T√§m√§ on testi."
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"LumiOpen/Viking-7B\")\n",
    "#tokenizer = MarianTokenizer.from_pretrained(\"/home/tommi/hf_testing/converted-vocab_fix_model\")\n",
    "model = MarianMTModel.from_pretrained(\"../converted-vocab_fix_model_viking7\").to(\"cpu\")\n",
    "# THIS IS THE CORRECT WAY: eos token at the end of input, pad at start\n",
    "input_ids = tokenizer(\"This is a test.</s>\",return_tensors=\"pt\").input_ids.to(\"cpu\")\n",
    "decoder_input_ids = tokenizer(\"<pad>\",return_tensors=\"pt\").input_ids.to(\"cpu\")\n",
    "for _ in range(100):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "        input_ids=input_ids,\n",
    "        decoder_input_ids=decoder_input_ids,\n",
    "    )\n",
    "        logits = outputs.logits\n",
    "        next_token_logits = logits[:, -1, :]\n",
    "        next_token_logits[0][tokenizer.pad_token_id] = float(\"-inf\")\n",
    "        next_token_id = torch.argmax(next_token_logits, dim=-1).unsqueeze(0)\n",
    "        decoder_input_ids = torch.cat([decoder_input_ids, next_token_id], dim=-1)\n",
    "        if next_token_id[0][0] == tokenizer.eos_token_id:\n",
    "            break\n",
    "        new_token = tokenizer.decode(next_token_id.squeeze(), skip_special_tokens=True)\n",
    "        print(new_token, end=\"\", flush=True)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
